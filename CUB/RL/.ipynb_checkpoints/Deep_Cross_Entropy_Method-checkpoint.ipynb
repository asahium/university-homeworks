{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_i1q1TWG9zH"
   },
   "source": [
    "# Deep Cross-Entropy Method, 8 pts + bonuses\n",
    "\n",
    "In this section we'll extend your CEM implementation with neural networks. You will train a multi-layer neural network to solve simple continuous state space games. __Please make sure you're done with tabular crossentropy method from another notebook.__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C2xd5vPwPVCb",
    "outputId": "b612b734-cd59-47d4-9ded-d70d87235bf7"
   },
   "outputs": [],
   "source": [
    "# Install necessary libraries. If you encounter difficulties in local installation, use google colab.\n",
    "\n",
    "!pip install swig\n",
    "!pip install gymnasium[toy_text,classic_control,box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-vU8V8BVMGP"
   },
   "source": [
    "Here we start with [CartPole-v1](https://gymnasium.farama.org/environments/classic_control/cart_pole/) environment. As usual, first of all read the description of the environment: what are the goals of the game, what are observations and actions, how reward is calculated, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "id": "_2zbc7ahG9zK",
    "outputId": "43e725c1-fe85-4b8c-dbc9-392a75cc88a4"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "plt.imshow(env.render())\n",
    "print(\"state vector dim =\", state_dim)\n",
    "print(\"n_actions =\", n_actions)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-0TXepuVvZ7"
   },
   "source": [
    "Let's play with the environment with random strategy and generate a video with results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iJBX88ZqNxKv",
    "outputId": "af9de6f7-db06-418f-ba64-3eb7f3d66214"
   },
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "with RecordVideo(\n",
    "    env=gym.make(\"CartPole-v1\", render_mode=\"rgb_array\"),\n",
    "    video_folder=\"./videos\",\n",
    "    episode_trigger=lambda episode_number: True,\n",
    ") as env_monitor:\n",
    "\n",
    "    s, info = env_monitor.reset()\n",
    "    for t in range(100):\n",
    "        a = env_monitor.action_space.sample()\n",
    "        s, r, terminated, truncated, info = env_monitor.step(a)\n",
    "        if terminated or truncated:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "id": "p8AjZB7RV5nD",
    "outputId": "70eb84e2-59fe-4bea-dd10-720d05415912"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from base64 import b64encode\n",
    "from IPython.display import HTML\n",
    "\n",
    "video_paths = sorted([s for s in Path(\"videos\").iterdir() if s.suffix == \".mp4\"])\n",
    "video_path = video_paths[0]  # You can also try other indices\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # https://stackoverflow.com/a/57378660/1214547\n",
    "    with video_path.open(\"rb\") as fp:\n",
    "        mp4 = fp.read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "else:\n",
    "    data_url = str(video_path)\n",
    "\n",
    "HTML(\n",
    "    \"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\n",
    "        data_url\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z72_alhdG9zK"
   },
   "source": [
    "# Neural Network Policy\n",
    "\n",
    "For this assignment we'll utilize the simplified neural network implementation from __[Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)__. Here's what you'll need:\n",
    "\n",
    "* `agent.partial_fit(states, actions)` - make a single training pass over the data. Maximize the probability of :actions: from :states:\n",
    "* `agent.predict_proba(states)` - predict probabilities of all actions, a matrix of shape __[len(states), n_actions]__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "wLItY4unG9zL",
    "outputId": "a182582b-2a72-431b-f2bf-5be2a34c3398"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "agent = MLPClassifier(\n",
    "    hidden_layer_sizes=(20, 20),\n",
    "    activation=\"tanh\",\n",
    ")\n",
    "\n",
    "# initialize agent to the dimension of state space and number of actions\n",
    "agent.partial_fit([env.reset()[0]] * n_actions, range(n_actions), range(n_actions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eyFS3oUmG9zL"
   },
   "outputs": [],
   "source": [
    "def generate_session(env, agent, t_max=1000, test=False):\n",
    "    \"\"\"\n",
    "    Play a single game using agent neural network.\n",
    "    Terminate when game finishes or after :t_max: steps\n",
    "    \"\"\"\n",
    "    states, actions = [], []\n",
    "    total_reward = 0\n",
    "\n",
    "    s, info = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "\n",
    "        # use agent to predict a vector of action probabilities for state :s:\n",
    "        probs = <YOUR CODE>\n",
    "\n",
    "        assert probs.shape == (env.action_space.n,), \"make sure probabilities are a vector (hint: np.reshape)\"\n",
    "\n",
    "        # use the probabilities you predicted to pick an action\n",
    "        if test:\n",
    "            # on the test use the best (the most likely) actions at test\n",
    "            # experiment, will it work on the train and vice versa?\n",
    "            a = <YOUR CODE>\n",
    "            # ^-- hint: try np.argmax\n",
    "        else:\n",
    "            # sample proportionally to the probabilities,\n",
    "            # don't just take the most likely action at train\n",
    "            a = <YOUR CODE>\n",
    "            # ^-- hint: try np.random.choice        \n",
    "        \n",
    "        new_s, r, terminated, truncated, info = env.step(a)\n",
    "\n",
    "        # record sessions like you did before\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward += r\n",
    "\n",
    "        s = new_s\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "            \n",
    "    return states, actions, total_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4xgrTCgJG9zL",
    "outputId": "ac8d5b83-6d65-4128-b95b-1ea0338c7993"
   },
   "outputs": [],
   "source": [
    "dummy_states, dummy_actions, dummy_reward = generate_session(env, agent, t_max=5)\n",
    "print(\"states:\", np.stack(dummy_states))\n",
    "print(\"actions:\", dummy_actions)\n",
    "print(\"reward:\", dummy_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "0b4vAareRRo7",
    "outputId": "b77925f9-6a2e-491c-f2d1-2660e9597311"
   },
   "outputs": [],
   "source": [
    "# let's see the initial reward distribution\n",
    "sample_rewards = [generate_session(env, agent, t_max=1000, test=False)[-1] for _ in range(200)]\n",
    "\n",
    "plt.hist(sample_rewards, bins=20)\n",
    "plt.vlines([np.percentile(sample_rewards, 50)], [0], [100], label=\"50'th percentile\", color='green')\n",
    "plt.vlines([np.percentile(sample_rewards, 90)], [0], [100], label=\"90'th percentile\", color='red')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p85lt16qG9zL"
   },
   "source": [
    "### CEM steps\n",
    "Deep CEM uses exactly the same strategy as the regular CEM, so you can copy your function code from previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4On-p7p4G9zL"
   },
   "outputs": [],
   "source": [
    "def select_elites(states_batch, actions_batch, rewards_batch, percentile=50):\n",
    "    \"\"\"\n",
    "    Select states and actions from games that have rewards >= percentile\n",
    "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
    "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
    "    :param rewards_batch: list of rewards, rewards_batch[session_i]\n",
    "\n",
    "    :returns: elite_states,elite_actions, both 1D lists of states and respective actions from elite sessions\n",
    "\n",
    "    Please return elite states and actions in their original order\n",
    "    [i.e. sorted by session number and timestep within session]\n",
    "\n",
    "    If you are confused, see examples below. Please don't assume that states are integers\n",
    "    (they will become different later).\n",
    "    \"\"\"\n",
    "\n",
    "    <YOUR CODE>\n",
    "\n",
    "    return elite_states, elite_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xc40V4DaG9zM"
   },
   "source": [
    "# Training loop\n",
    "Generate sessions, select N best and fit to those. Here we don't need to solve the environment with the best possible quality. Just reaching a mean reward 190 is enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 877
    },
    "id": "euK7WRQiG9zM",
    "outputId": "69b6b7f2-d9dc-44c0-a617-962c88dae140"
   },
   "outputs": [],
   "source": [
    "n_sessions = 20\n",
    "percentile = 50\n",
    "log = []\n",
    "\n",
    "for i in range(n_sessions):\n",
    "    # generate new sessions\n",
    "    sessions = [ < generate a list of n_sessions new sessions > ]\n",
    "\n",
    "    states_batch, actions_batch, rewards_batch = map(np.array, zip(*sessions))\n",
    "\n",
    "    < estimate mean reward and print >\n",
    "    \n",
    "    elite_states, elite_actions = <select elite actions just like before>\n",
    "\n",
    "    <partial_fit agent to predict elite_actions (y) from elite_states (X)>\n",
    "    \n",
    "    if mean_reward > 190:\n",
    "        print(\"You Win! You may stop training now via KeyboardInterrupt.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeNWKjtsG9zM"
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_x0qmxV3W0MI"
   },
   "source": [
    "Let's generate a video for our trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FsvTmFhB45Sw",
    "outputId": "a7e9765c-6947-4a2a-955b-24f5487905be"
   },
   "outputs": [],
   "source": [
    "# Record sessions\n",
    "\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "with RecordVideo(\n",
    "    env=gym.make(\"CartPole-v1\", render_mode=\"rgb_array\"),\n",
    "    video_folder=\"./videos\",\n",
    "    episode_trigger=lambda episode_number: True,\n",
    ") as env_monitor:\n",
    "    sessions = [generate_session(env_monitor, agent) for _ in range(5)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "id": "kLPXdME7G9zN",
    "outputId": "6a9b2990-d9bf-4b4c-c92b-5526c976e79b"
   },
   "outputs": [],
   "source": [
    "# Show video\n",
    "\n",
    "from pathlib import Path\n",
    "from base64 import b64encode\n",
    "from IPython.display import HTML\n",
    "\n",
    "video_paths = sorted([s for s in Path(\"videos\").iterdir() if s.suffix == \".mp4\"])\n",
    "video_path = video_paths[0]  # You can also try other indices\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # https://stackoverflow.com/a/57378660/1214547\n",
    "    with video_path.open(\"rb\") as fp:\n",
    "        mp4 = fp.read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "else:\n",
    "    data_url = str(video_path)\n",
    "\n",
    "HTML(\n",
    "    \"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\n",
    "        data_url\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LpAJc4rG9zN"
   },
   "source": [
    "# The Assignment\n",
    "\n",
    "### Deep Cross-Entropy Method\n",
    "\n",
    "By this moment, you should have got enough score on CartPole-v1 to consider it solved. It's time to try something harder.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "* __2.1__ (**3pts**) Pick one of environments: [MountainCar-v0](https://gymnasium.farama.org/environments/classic_control/mountain_car/) or [LunarLander-v2](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
    "  * For MountainCar, get average reward of __at least -150__\n",
    "  * For LunarLander, get average reward of __at least +50__\n",
    "\n",
    "See the tips section below, it's kinda important.\n",
    "__Note:__ If your agent is below the target score, you'll still get some of the points depending on the result.\n",
    "  \n",
    "  \n",
    "* __2.2__ Devise a way to speed up training against the default version\n",
    "  * (**2pts**) Try re-using samples from 3-5 last iterations when computing threshold and training.\n",
    "  * (**3pts**) Obtain **-100** at MountainCar-v0 or **+200** at LunarLander-v2.Feel free to experiment with hyperparameters, architectures, schedules etc.\n",
    "    \n",
    "  \n",
    "### Tips\n",
    "* Sessions for MountainCar may last for 10k+ ticks. Make sure ```t_max``` param is at least 10k.\n",
    " * Also it may be a good idea to cut rewards via \">\" and not \">=\". If 90% of your sessions get reward of -10k and 10% are better, than if you use percentile 20% as threshold, R >= threshold __fails to cut off bad sessions__ while R > threshold works alright.\n",
    "* If it doesn't train, it's a good idea to plot reward distribution and record sessions: they may give you some clue.\n",
    "* 20-neuron network is probably not enough, feel free to experiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dzk41lDPG9zO"
   },
   "source": [
    "### Bonus tasks (Up to 6 points)\n",
    "\n",
    "* __2.3 bonus__ (2 pts) Try to find a network architecture and training params that solve __both__ environments above\n",
    "\n",
    "* __2.4 bonus__ (4 pts) Solve continuous action space task with `MLPRegressor` or similar.\n",
    "  * Since your agent only predicts the \"expected\" action, you will have to add noise to ensure exploration.\n",
    "  * Choose one of [MountainCarContinuous-v0](https://gymnasium.farama.org/environments/classic_control/mountain_car_continuous/), [LunarLanderContinuous-v2](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
    "  * Slightly less points for getting some results below solution threshold. Note that discrete and continuous environments may have slightly different rules aside from action spaces.\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
