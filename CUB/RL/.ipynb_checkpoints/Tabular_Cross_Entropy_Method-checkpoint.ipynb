{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33b4s8uvi0J2"
   },
   "source": [
    "# Tabular Cross-Entropy Method (CEM), 2 pts\n",
    "\n",
    "In this notebook you will play with a simple reinforcement learning environment and learn a policy using Cross-Entropy method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UB4IExOoi2gR",
    "outputId": "7d4e0466-4e6d-4afa-e22c-99c9d6cae4f8"
   },
   "outputs": [],
   "source": [
    "# Install necessary libraries. If you encounter difficulties in local installation, use google colab.\n",
    "\n",
    "!pip install swig\n",
    "!pip install gymnasium[toy-text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGR84QM86I-1"
   },
   "source": [
    "In this notebook we will use **Taxi** environment from Gymnasium library. First of all, read the [description](https://gymnasium.farama.org/environments/toy_text/taxi/) of this environment, try to understand the goal of the game, possible states and actions in the game and how the reward is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "id": "e43-p8wo5zE6",
    "outputId": "306fc93a-330c-429f-af35-40efcbf37a9b"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create environment\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n",
    "\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "print(f\"n_states = {n_states}, n_actions = {n_actions}\")\n",
    "\n",
    "# Initialize environment\n",
    "state, info = env.reset(seed=42)\n",
    "\n",
    "# Show the current state\n",
    "plt.imshow(env.render())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XiuVJQEY9kWM"
   },
   "source": [
    "Let's play with the environment with random strategy and generate a video with results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zreb4mzai0J4",
    "outputId": "4649909e-e096-4ceb-cdd3-f8020b4f4c66"
   },
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "with RecordVideo(\n",
    "    env=gym.make(\"Taxi-v3\", render_mode=\"rgb_array\"),\n",
    "    video_folder=\"./videos\",\n",
    "    episode_trigger=lambda episode_number: True,\n",
    ") as env_monitor:\n",
    "\n",
    "    s, info = env_monitor.reset()\n",
    "    for t in range(100):\n",
    "        a = env_monitor.action_space.sample()\n",
    "        s, r, terminated, truncated, info = env_monitor.step(a)\n",
    "        if terminated or truncated:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "id": "fQrNTyrMGs2F",
    "outputId": "3ee249e8-09ad-49ec-f3ac-d55fd4847973"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from base64 import b64encode\n",
    "from IPython.display import HTML\n",
    "\n",
    "video_paths = sorted([s for s in Path(\"videos\").iterdir() if s.suffix == \".mp4\"])\n",
    "video_path = video_paths[0]  \n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # https://stackoverflow.com/a/57378660/1214547\n",
    "    with video_path.open(\"rb\") as fp:\n",
    "        mp4 = fp.read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "else:\n",
    "    data_url = str(video_path)\n",
    "\n",
    "HTML(\n",
    "    \"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\n",
    "        data_url\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4itsZrti0J7"
   },
   "source": [
    "# Create stochastic policy\n",
    "\n",
    "Our policy is a probability distribution:\n",
    "\n",
    "```policy[s,a] = P(take action a | in state s)```\n",
    "\n",
    "Since we use integer state and action representations, you can use a 2-dimensional array to represent the policy.\n",
    "\n",
    "Please initialize policy __uniformly__, that is, probabililities of all actions should be equal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aj3MuEqgi0J7"
   },
   "outputs": [],
   "source": [
    "policy = <your code here! Create an array to store action probabilities>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KeZTJGtbi0J8"
   },
   "outputs": [],
   "source": [
    "# tests for uniform policy\n",
    "\n",
    "assert type(policy) in (np.ndarray, np.matrix)\n",
    "assert np.allclose(policy, 1./n_actions)\n",
    "assert np.allclose(np.sum(policy, axis=1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9G643X4ci0J8"
   },
   "source": [
    "# Play the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQfs4Frpi0J9"
   },
   "outputs": [],
   "source": [
    "def generate_session(env, policy, t_max=10**4, test=False):\n",
    "    \"\"\"\n",
    "    Play game until end or for t_max ticks.\n",
    "    :param env: gymnasium environment\n",
    "    :param policy: an array of shape [n_states,n_actions] with action probabilities\n",
    "    :returns: list of states, list of actions and sum of rewards\n",
    "    \"\"\"\n",
    "    states, actions = [], []\n",
    "    total_reward = 0.\n",
    "\n",
    "    s, info = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "\n",
    "        # use the probabilities you predicted to pick an action\n",
    "        if test:\n",
    "            # on the test use the best (the most likely) actions at test\n",
    "            # experiment\n",
    "            # to account for eligible steps you may use info['action_mask']\n",
    "\n",
    "            a = < your code >\n",
    "\n",
    "            # # ^-- hint: try np.argmax\n",
    "        else:\n",
    "            # sample proportionally to the probabilities,\n",
    "            # don't just take the most likely action at train\n",
    "            a = < your code >\n",
    "            # ^-- hint: try np.random.choice\n",
    "\n",
    "        new_s, r, terminated, truncated, info = env.step(a)\n",
    "\n",
    "        # record sessions\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward += r\n",
    "\n",
    "        s = new_s\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    return states, actions, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JxzRJKYpi0J9"
   },
   "outputs": [],
   "source": [
    "# tests for generate_session\n",
    "\n",
    "s, a, r = generate_session(env, policy)\n",
    "assert type(s) == type(a) == list\n",
    "assert len(s) == len(a)\n",
    "assert type(r) == float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "RWFfo5mIi0J9",
    "outputId": "f71f76e0-4edf-486d-b1e6-1e116d28059d"
   },
   "outputs": [],
   "source": [
    "# let's see the initial reward distribution\n",
    "sample_rewards = [generate_session(env, policy, t_max=1000)[-1] for _ in range(200)]\n",
    "\n",
    "plt.hist(sample_rewards, bins=20)\n",
    "plt.vlines([np.percentile(sample_rewards, 50)], [0], [100], label=\"50'th percentile\", color='green')\n",
    "plt.vlines([np.percentile(sample_rewards, 90)], [0], [100], label=\"90'th percentile\", color='red')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUcJwBmji0J9"
   },
   "source": [
    "### Cross-entropy method steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZ1OEnxii0J9"
   },
   "outputs": [],
   "source": [
    "def select_elites(states_batch, actions_batch, rewards_batch, percentile=50):\n",
    "    \"\"\"\n",
    "    Select states and actions from games that have rewards >= percentile\n",
    "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
    "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
    "    :param rewards_batch: list of rewards, rewards_batch[session_i]\n",
    "\n",
    "    :returns: elite_states, elite_actions, both 1D lists of states and respective actions from elite sessions\n",
    "\n",
    "    Please return elite states and actions in their original order\n",
    "    [i.e. sorted by session number and timestep within session]\n",
    "    \"\"\"\n",
    "\n",
    "    # <Compute minimum reward for elite sessions. Hint: use np.percentile >\n",
    "    \n",
    "    < Your Code >\n",
    "\n",
    "    return elite_states, elite_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RhxFEL7Ti0J-",
    "outputId": "6091dd3a-1c99-4a70-bb6a-2c4074a5fa09"
   },
   "outputs": [],
   "source": [
    "# tests for select_elites\n",
    "\n",
    "states_batch = [\n",
    "    [1, 2, 3],     # game1\n",
    "    [4, 2, 0, 2],  # game2\n",
    "    [3, 1],        # game3\n",
    "]\n",
    "\n",
    "actions_batch = [\n",
    "    [0, 2, 4],     # game1\n",
    "    [3, 2, 0, 1],  # game2\n",
    "    [3, 3],        # game3\n",
    "]\n",
    "rewards_batch = [\n",
    "    3,  # game1\n",
    "    4,  # game2\n",
    "    5,  # game3\n",
    "]\n",
    "\n",
    "test_result_0 = select_elites(\n",
    "    states_batch, actions_batch, rewards_batch, percentile=0)\n",
    "test_result_30 = select_elites(\n",
    "    states_batch, actions_batch, rewards_batch, percentile=30)\n",
    "test_result_90 = select_elites(\n",
    "    states_batch, actions_batch, rewards_batch, percentile=90)\n",
    "test_result_100 = select_elites(\n",
    "    states_batch, actions_batch, rewards_batch, percentile=100)\n",
    "\n",
    "assert np.all(test_result_0[0] == [1, 2, 3, 4, 2, 0, 2, 3, 1])  \\\n",
    "    and np.all(test_result_0[1] == [0, 2, 4, 3, 2, 0, 1, 3, 3]),\\\n",
    "    \"For percentile 0 you should return all states and actions in chronological order\"\n",
    "assert np.all(test_result_30[0] == [4, 2, 0, 2, 3, 1]) and \\\n",
    "    np.all(test_result_30[1] == [3, 2, 0, 1, 3, 3]),\\\n",
    "    \"For percentile 30 you should only select states/actions from two first\"\n",
    "assert np.all(test_result_90[0] == [3, 1]) and \\\n",
    "    np.all(test_result_90[1] == [3, 3]),\\\n",
    "    \"For percentile 90 you should only select states/actions from one game\"\n",
    "assert np.all(test_result_100[0] == [3, 1]) and\\\n",
    "    np.all(test_result_100[1] == [3, 3]),\\\n",
    "    \"Please make sure you use >=, not >. Also double-check how you compute percentile.\"\n",
    "print(\"Ok!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iYaA-VDai0J-"
   },
   "outputs": [],
   "source": [
    "def update_policy(elite_states, elite_actions):\n",
    "    \"\"\"\n",
    "    Given old policy and a list of elite states/actions from select_elites,\n",
    "    return new updated policy where each action probability is proportional to\n",
    "\n",
    "    policy[s_i,a_i] ~ #[occurences of si and ai in elite states/actions]\n",
    "\n",
    "    Don't forget to normalize policy to get valid probabilities and handle 0/0 case.\n",
    "    In case you never visited a state, set probabilities for all actions to 1./n_actions\n",
    "\n",
    "    :param elite_states: 1D list of states from elite sessions\n",
    "    :param elite_actions: 1D list of actions from elite sessions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    new_policy = np.zeros([n_states, n_actions])\n",
    "    \n",
    "    <Your code here: update probabilities for actions given elite states & actions >\n",
    "    # Don't forget to set 1/n_actions for all actions in unvisited states.\n",
    "\n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EKRTOuxCi0J-",
    "outputId": "4b407d8b-04f3-4ca4-aec6-79e40c60b6a3"
   },
   "outputs": [],
   "source": [
    "# tests for update_policy\n",
    "\n",
    "elite_states = [1, 2, 3, 4, 2, 0, 2, 3, 1]\n",
    "elite_actions = [0, 2, 4, 3, 2, 0, 1, 3, 3]\n",
    "\n",
    "new_policy = update_policy(elite_states, elite_actions)\n",
    "\n",
    "assert np.isfinite(new_policy).all(\n",
    "), \"Your new policy contains NaNs or +-inf. Make sure you don't divide by zero.\"\n",
    "assert np.all(\n",
    "    new_policy >= 0), \"Your new policy can't have negative action probabilities\"\n",
    "assert np.allclose(new_policy.sum(\n",
    "    axis=-1), 1), \"Your new policy should be a valid probability distribution over actions\"\n",
    "reference_answer = np.array([\n",
    "    [1.,  0.,  0.,  0.,  0.],\n",
    "    [0.5,  0.,  0.,  0.5,  0.],\n",
    "    [0.,  0.33333333,  0.66666667,  0.,  0.],\n",
    "    [0.,  0.,  0.,  0.5,  0.5]])\n",
    "assert np.allclose(new_policy[:4, :5], reference_answer)\n",
    "print(\"Ok!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmIJBA7si0J_"
   },
   "source": [
    "# Training loop\n",
    "Generate sessions, select N best and fit to those. Monitor the mean reward in iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fDE9GrOi0J_"
   },
   "outputs": [],
   "source": [
    "# reset policy just in case\n",
    "policy = np.ones([n_states, n_actions]) / n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gRbpEHN_i0J_",
    "outputId": "b96a963f-cbba-4b2e-d9cc-8482a1ac6241"
   },
   "outputs": [],
   "source": [
    "n_sessions = 500  # sample this many sessions\n",
    "percentile = 10  # take this percent of session with highest rewards\n",
    "learning_rate = 0.5  # add this thing to all counts for stability\n",
    "\n",
    "log = []\n",
    "\n",
    "for i in range(20):\n",
    "\n",
    "    sessions = [ < generate a list of n_sessions new sessions > ]\n",
    "    states_batch, actions_batch, rewards_batch = zip(*sessions)\n",
    "    \n",
    "    < compute mean reward for the current iteration and print >\n",
    "    \n",
    "    elite_states, elite_actions = < select elite states/actions >\n",
    "    \n",
    "    new_policy = < compute new policy >\n",
    "    policy = (1 - learning_rate)*policy + learning_rate*new_policy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBR-5_-IIwzp"
   },
   "source": [
    "Let's see the distribution of rewards for the found policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "8llc5Ghwi0J_",
    "outputId": "6ae78f9d-09b2-4d40-9ba4-bfead0419582"
   },
   "outputs": [],
   "source": [
    "sample_rewards = [generate_session(env, policy, t_max=1000, test=True)[-1] for _ in range(200)]\n",
    "\n",
    "plt.hist(sample_rewards, bins=20)\n",
    "plt.vlines([np.percentile(sample_rewards, 50)], [0], [100], label=\"50'th percentile\", color='green')\n",
    "plt.vlines([np.percentile(sample_rewards, 90)], [0], [100], label=\"90'th percentile\", color='red')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KyGgPrYKG18"
   },
   "source": [
    "Let's generate a video for our trained policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_TmFlNk1EOmC",
    "outputId": "1dd2f9e5-58d3-43c6-a8ae-8ba8f59ad5ee"
   },
   "outputs": [],
   "source": [
    "# Record sessions\n",
    "\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "with RecordVideo(\n",
    "    env=gym.make(\"Taxi-v3\", render_mode=\"rgb_array\"),\n",
    "    video_folder=\"./videos\",\n",
    "    episode_trigger=lambda episode_number: True,\n",
    ") as env_monitor:\n",
    "    sessions = [generate_session(env_monitor, policy) for _ in range(5)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "id": "Df_XLFIfEXN2",
    "outputId": "d9549b34-a3fe-45d3-d2a6-19711a88e39e"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from base64 import b64encode\n",
    "from IPython.display import HTML\n",
    "\n",
    "video_paths = sorted([s for s in Path(\"videos\").iterdir() if s.suffix == \".mp4\"])\n",
    "video_path = video_paths[0]  # You can also try other indices\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # https://stackoverflow.com/a/57378660/1214547\n",
    "    with video_path.open(\"rb\") as fp:\n",
    "        mp4 = fp.read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "else:\n",
    "    data_url = str(video_path)\n",
    "\n",
    "HTML(\n",
    "    \"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\n",
    "        data_url\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFnYMz4Ui0J_"
   },
   "source": [
    "### Reflecting on results\n",
    "\n",
    "You may have noticed that the taxi problem quickly converges from <-1000 to a near-optimal score and then descends back into -50/-100. This is in part because the environment has some innate randomness. Namely, the starting points of passenger/driver change from episode to episode.\n",
    "\n",
    "In case CEM failed to learn how to win from one distinct starting point, it will simply discard it because no sessions from that starting point will make it into the \"elites\".\n",
    "\n",
    "To mitigate that problem, you can either reduce the threshold for elite sessions (duct tape way) or  change the way you evaluate strategy (theoretically correct way). You can first sample an action for every possible state and then evaluate this choice of actions by running _several_ games and averaging rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWljrqWX14GV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
