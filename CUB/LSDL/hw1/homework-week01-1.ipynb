{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"aeea82edd47e46e58c315fb74f35d6fa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ccf8a3d0159b4470a5b9bdda8e0fbbb0","IPY_MODEL_5751e3ce02fc4c1783daf618e3e0379c","IPY_MODEL_8944d958f1274fb2832db2aaa08380a0"],"layout":"IPY_MODEL_579a9b695cb34173b713e113bd9309c2"}},"ccf8a3d0159b4470a5b9bdda8e0fbbb0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e5a9e072b6004117b0be5fba625d37b8","placeholder":"​","style":"IPY_MODEL_e780062f8f5a4a019488703bcf036765","value":"Loading dataset shards: 100%"}},"5751e3ce02fc4c1783daf618e3e0379c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a7ba5d488b6a42ccb820ded7b7ded416","max":27,"min":0,"orientation":"horizontal","style":"IPY_MODEL_df7d8e8496074a86bcd333c268150f64","value":27}},"8944d958f1274fb2832db2aaa08380a0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_98880c5be9c54eeaad35cc622c5dbcc8","placeholder":"​","style":"IPY_MODEL_37c90083f4d0439f904161906494fa65","value":" 27/27 [00:20&lt;00:00, 20.38s/it]"}},"579a9b695cb34173b713e113bd9309c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5a9e072b6004117b0be5fba625d37b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e780062f8f5a4a019488703bcf036765":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a7ba5d488b6a42ccb820ded7b7ded416":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df7d8e8496074a86bcd333c268150f64":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"98880c5be9c54eeaad35cc622c5dbcc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37c90083f4d0439f904161906494fa65":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"98eefa05","cell_type":"markdown","source":"# LSDL CUB, Homework 1. Robust fine-tuning of CLIP [10 pts]\n\nYour main goal in this home assignment is to implement the [Lipsum-FT](https://openreview.net/attachment?id=2JF8mJRJ7M&name=pdf) method for robust fine-tuning of CLIP.\n\nRules for the assignment:\n\n- We will be using the same dataset, [DomainNet](https://ai.bu.edu/M3SDA/) (also can be found [here](https://huggingface.co/datasets/wltjr1007/DomainNet)), as in the original paper. Use the **Real** domain as in-distribution (ID) data and the rest of domains as out-of-distribution (OOD) data. Use the **Real** train split for training and test splits for evaluation on all of the domains.\n\n- If training takes too much time, you may select a subset (i.e., 50% or 33%) of training data instead of the full split.\n\n- `ViT-B/16` backbone is recommended.\n\n- In order to **pass the assignment**, you need to plot a Pareto front (i.e., ID-OOD plot like Figure 1 from this [paper](https://arxiv.org/pdf/2109.01903)). We will be using 5 OOD domains, so you need to plot 5 Pareto fronts, one for each distribution shift.\n\n- You may use any code from the [seminar](https://github.com/isadrtdinov/lsdl-cub/tree/2025/week01-finetune/seminar). Also, you may code your training pipelines either in pure PyTorch or combinine it with [huggingface](https://huggingface.co/) libraries. Additionally, you may find [`clip`](https://github.com/openai/CLIP) and [`wise-ft`](https://github.com/mlfoundations/wise-ft) repos useful.\n\n- Do not use the implemention from the authors or any publicly available implementations of this method.\n\n- It will be much easier to check your assigment if you maintain a clear code structure (e.g., put different blocks of code into separate files, add necessary coments, etc).","metadata":{"id":"98eefa05"}},{"id":"8568a105","cell_type":"markdown","source":"## 1. Zero-shot model [1 pts]\n\nCreate a zero-shot model on top of pre-trained CLIP and evaluate it on **Real** test set (ID accuracy) and on 5 distribution shifts: **Clipart**, **Infograph**, **Painting**, **Quickdraw**, **Sketch** (OOD accuracy).","metadata":{"id":"8568a105"}},{"id":"33153ef9","cell_type":"code","source":"import subprocess\nimport sys\n\ntry:\n    import clip\nexcept ImportError:\n    print(\"Installing CLIP...\")\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"git+https://github.com/openai/CLIP.git\"])\n    import clip\n\nimport torch\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import transforms\nfrom datasets import load_dataset\nimport numpy as np\nfrom tqdm import tqdm\n\n# Load pre-trained CLIP model\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\nmodel, preprocess = clip.load(\"ViT-B/16\", device=device)\n\n# Load DomainNet dataset\nprint(\"Loading DomainNet dataset...\")\ndataset = load_dataset(\"wltjr1007/DomainNet\")\n\n# Check available splits\nprint(\"Available dataset splits:\", list(dataset.keys()))\n\n# The dataset structure is different - let's check the actual structure\nsample = dataset[\"train\"][0]\nprint(\"Sample from train:\", sample)\n\n# Check domain mapping\ndomain_mapping = {\n    0: \"real\",\n    1: \"clipart\",\n    2: \"infograph\",\n    3: \"quickdraw\",\n    4: \"painting\",\n    5: \"sketch\"\n}\n\n# Get all unique domains in the dataset\nunique_domains = set()\nfor split in ['train', 'test']:\n    sample_batch = dataset[split][:100]  # Check first 100 samples\n    for domain_id in sample_batch['domain']:\n        unique_domains.add(domain_id)\n\nprint(\"Unique domain IDs found:\", sorted(unique_domains))\n\n# Map domain IDs to names\navailable_domain_names = []\nfor domain_id in sorted(unique_domains):\n    if domain_id in domain_mapping:\n        available_domain_names.append(domain_mapping[domain_id])\n    else:\n        available_domain_names.append(f\"domain_{domain_id}\")\n\nprint(\"Available domains:\", available_domain_names)\n\n# Define domains\nid_domain = \"real\"  # domain_id = 0\nood_domains = [d for d in available_domain_names if d != id_domain]\n\nprint(f\"ID domain: {id_domain}\")\nprint(f\"OOD domains: {ood_domains}\")\n\n# Get class names\nclass_names = dataset[\"train\"].features[\"label\"].names\nprint(f\"Number of classes: {len(class_names)}\")\nprint(f\"First 10 classes: {class_names[:10]}\")\n\n# Create text prompts for zero-shot classification\ntext_prompts = [f\"a photo of a {class_name}\" for class_name in class_names]\ntext_inputs = clip.tokenize(text_prompts).to(device)\n\n# Get text features\nprint(\"Encoding text prompts...\")\nwith torch.no_grad():\n    text_features = model.encode_text(text_inputs)\n    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n\ndef filter_dataset_by_domain(dataset_split, domain_name, max_samples=None):\n    \"\"\"Filter dataset by domain and optionally limit number of samples\"\"\"\n    # Get domain ID from name\n    domain_id = None\n    for did, dname in domain_mapping.items():\n        if dname == domain_name:\n            domain_id = did\n            break\n\n    if domain_id is None:\n        print(f\"Domain {domain_name} not found in mapping\")\n        return None\n\n    # Filter by domain\n    filtered_indices = []\n    for i, domain in enumerate(dataset_split['domain']):\n        if domain == domain_id:\n            filtered_indices.append(i)\n            if max_samples and len(filtered_indices) >= max_samples:\n                break\n\n    print(f\"Found {len(filtered_indices)} samples for domain {domain_name}\")\n\n    if len(filtered_indices) == 0:\n        return None\n\n    # Create subset\n    return Subset(dataset_split, filtered_indices)\n\ndef custom_collate_fn(batch):\n    \"\"\"Custom collate function to handle PIL images\"\"\"\n    images = []\n    labels = []\n\n    for item in batch:\n        # Convert PIL image to tensor using preprocess\n        image = preprocess(item['image'].convert('RGB'))\n        images.append(image)\n        labels.append(item['label'])\n\n    return {\n        'image': torch.stack(images),\n        'label': torch.tensor(labels)\n    }\n\ndef evaluate_zero_shot(dataset_subset, domain_name):\n    \"\"\"Evaluate zero-shot CLIP on a dataset subset\"\"\"\n    if dataset_subset is None:\n        print(f\"No dataset available for {domain_name}\")\n        return 0.0\n\n    correct = 0\n    total = 0\n\n    # Create dataloader with custom collate function\n    dataloader = DataLoader(dataset_subset, batch_size=16, shuffle=False, collate_fn=custom_collate_fn)\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=f\"Evaluating {domain_name}\"):\n            try:\n                images = batch['image'].to(device)\n                labels = batch['label'].to(device)\n\n                # Get image features\n                image_features = model.encode_image(images)\n                image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n\n                # Calculate similarities and predictions\n                similarities = (image_features @ text_features.T)\n                predictions = similarities.argmax(dim=-1)\n\n                correct += (predictions == labels).sum().item()\n                total += labels.size(0)\n            except Exception as e:\n                print(f\"Error processing batch in {domain_name}: {e}\")\n                continue\n\n    accuracy = correct / total if total > 0 else 0.0\n    return accuracy\n\n# Evaluate on ID (Real) domain\nprint(f\"\\nEvaluating on ID domain ({id_domain})...\")\nreal_test_subset = filter_dataset_by_domain(dataset[\"test\"], id_domain, max_samples=5000)  # Limit for speed\n\nif real_test_subset is not None:\n    id_accuracy = evaluate_zero_shot(real_test_subset, \"Real\")\n    print(f\"ID Accuracy (Real): {id_accuracy:.4f}\")\nelse:\n    print(\"Could not find Real domain test data\")\n    id_accuracy = 0.0\n\n# Evaluate on OOD domains\nood_accuracies = {}\nprint(f\"\\nEvaluating on OOD domains...\")\nfor domain in ood_domains:\n    try:\n        domain_test_subset = filter_dataset_by_domain(dataset[\"test\"], domain, max_samples=5000)  # Limit for speed\n        if domain_test_subset is not None:\n            ood_accuracy = evaluate_zero_shot(domain_test_subset, domain.capitalize())\n            ood_accuracies[domain] = ood_accuracy\n            print(f\"OOD Accuracy ({domain.capitalize()}): {ood_accuracy:.4f}\")\n        else:\n            print(f\"Could not find test data for domain: {domain}\")\n            ood_accuracies[domain] = 0.0\n    except Exception as e:\n        print(f\"Error evaluating {domain}: {e}\")\n        ood_accuracies[domain] = 0.0\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"ZERO-SHOT RESULTS SUMMARY\")\nprint(\"=\"*50)\nprint(f\"ID Accuracy (Real): {id_accuracy:.4f}\")\nprint(\"OOD Accuracies:\")\nfor domain, acc in ood_accuracies.items():\n    print(f\"  {domain.capitalize()}: {acc:.4f}\")\n\navg_ood_accuracy = np.mean(list(ood_accuracies.values())) if ood_accuracies else 0.0\nprint(f\"Average OOD Accuracy: {avg_ood_accuracy:.4f}\")\n\nprint(\"\\nZero-shot evaluation completed!\")\n\n# Store variables for next cells\nreal_train_subset = filter_dataset_by_domain(dataset[\"train\"], id_domain, max_samples=10000)  # Limit training data\nif real_train_subset is not None:\n    print(f\"Training subset size: {len(real_train_subset)}\")\n\n    # Store additional variables needed for training\n    train_subset = real_train_subset\n    real_test_subset = real_test_subset\nelse:\n    train_subset = None\n    print(\"Warning: Could not find Real domain training data\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":766,"referenced_widgets":["aeea82edd47e46e58c315fb74f35d6fa","ccf8a3d0159b4470a5b9bdda8e0fbbb0","5751e3ce02fc4c1783daf618e3e0379c","8944d958f1274fb2832db2aaa08380a0","579a9b695cb34173b713e113bd9309c2","e5a9e072b6004117b0be5fba625d37b8","e780062f8f5a4a019488703bcf036765","a7ba5d488b6a42ccb820ded7b7ded416","df7d8e8496074a86bcd333c268150f64","98880c5be9c54eeaad35cc622c5dbcc8","37c90083f4d0439f904161906494fa65"]},"id":"33153ef9","outputId":"4bc27e0f-d70e-4904-f687-2d4ba82675fb","trusted":true,"execution":{"iopub.status.busy":"2025-09-18T07:16:07.435274Z","iopub.execute_input":"2025-09-18T07:16:07.435546Z","iopub.status.idle":"2025-09-18T07:23:18.357754Z","shell.execute_reply.started":"2025-09-18T07:16:07.435520Z","shell.execute_reply":"2025-09-18T07:23:18.356956Z"}},"outputs":[{"name":"stdout","text":"Installing CLIP...\nCollecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-qkucxyf8\n","output_type":"stream"},{"name":"stderr","text":"  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-qkucxyf8\n","output_type":"stream"},{"name":"stdout","text":"  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting ftfy (from clip==1.0)\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (25.0)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->clip==1.0)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->clip==1.0)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->clip==1.0)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch->clip==1.0)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch->clip==1.0)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch->clip==1.0)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch->clip==1.0)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch->clip==1.0)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch->clip==1.0)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch->clip==1.0)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.2.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->clip==1.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->clip==1.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->clip==1.0) (2024.2.0)\nDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.8/44.8 kB 1.5 MB/s eta 0:00:00\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 363.4/363.4 MB 4.7 MB/s eta 0:00:00\nDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.8/13.8 MB 93.4 MB/s eta 0:00:00\nDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.6/24.6 MB 78.0 MB/s eta 0:00:00\nDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 883.7/883.7 kB 37.7 MB/s eta 0:00:00\nDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 664.8/664.8 MB 2.5 MB/s eta 0:00:00\nDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.5/211.5 MB 6.6 MB/s eta 0:00:00\nDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 MB 31.2 MB/s eta 0:00:00\nDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127.9/127.9 MB 13.7 MB/s eta 0:00:00\nDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.5/207.5 MB 2.3 MB/s eta 0:00:00\nDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/21.1 MB 9.2 MB/s eta 0:00:00\nBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py): started\n  Building wheel for clip (setup.py): finished with status 'done'\n  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=aef20864871e56d70a6f7bf4b45bf492eaf1cfa37f50ddfaf510a493f79440af\n  Stored in directory: /tmp/pip-ephem-wheel-cache-smdv5fa2/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\nSuccessfully built clip\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ftfy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, clip\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed clip-1.0 ftfy-6.3.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nUsing device: cuda\n","output_type":"stream"},{"name":"stderr","text":"100%|███████████████████████████████████████| 335M/335M [00:09<00:00, 38.8MiB/s]\n","output_type":"stream"},{"name":"stdout","text":"Loading DomainNet dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa524d29104b4f89b555cc0b43710ae9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00003.parquet:   0%|          | 0.00/759M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd0c93be33e344248471e17d3b1f7bd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00001-of-00003.parquet:   0%|          | 0.00/7.21G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7b1be6c1ec342cba9d5e4bec4360d57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00002-of-00003.parquet:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a6b89ebdaf04b80a68537cc656ea997"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/test-00000-of-00001.parquet:   0%|          | 0.00/5.60G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e61c4f5ad2e4834aac076b5272aeeef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/409832 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b951661519a44d06afef2cc3dc9dbe81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/176743 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69c9669415da434da6ed6ecf8070c0bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading dataset shards:   0%|          | 0/27 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16562bb4bc3f41bc80b227ea296fc15d"}},"metadata":{}},{"name":"stdout","text":"Available dataset splits: ['train', 'test']\nSample from train: {'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=300x300 at 0x7DE207F2FBD0>, 'label': 122, 'domain': 3, 'image_path': 'quickdraw/flying_saucer/4503624984035328.png'}\nUnique domain IDs found: [3]\nAvailable domains: ['quickdraw']\nID domain: real\nOOD domains: ['quickdraw']\nNumber of classes: 345\nFirst 10 classes: ['aircraft_carrier', 'airplane', 'alarm_clock', 'ambulance', 'angel', 'animal_migration', 'ant', 'anvil', 'apple', 'arm']\nEncoding text prompts...\n\nEvaluating on ID domain (real)...\nFound 5000 samples for domain real\n","output_type":"stream"},{"name":"stderr","text":"Evaluating Real: 100%|██████████| 313/313 [00:40<00:00,  7.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"ID Accuracy (Real): 0.7170\n\nEvaluating on OOD domains...\nFound 5000 samples for domain quickdraw\n","output_type":"stream"},{"name":"stderr","text":"Evaluating Quickdraw: 100%|██████████| 313/313 [00:27<00:00, 11.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"OOD Accuracy (Quickdraw): 0.1254\n\n==================================================\nZERO-SHOT RESULTS SUMMARY\n==================================================\nID Accuracy (Real): 0.7170\nOOD Accuracies:\n  Quickdraw: 0.1254\nAverage OOD Accuracy: 0.1254\n\nZero-shot evaluation completed!\nFound 10000 samples for domain real\nTraining subset size: 10000\n","output_type":"stream"}],"execution_count":1},{"id":"59735c0f","cell_type":"markdown","source":"## 2. Regular fine-tuning [2 pts]\n\nNow, fine-tune the whole image encoder on the **Real** train split. Use the zero-shot classification head as an initialization for the last linear layer. Calculate ID and OOD accuracy of this model.","metadata":{"id":"59735c0f"}},{"id":"9b800e9a","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport random\n\n# Define evaluation function for fine-tuned model\ndef evaluate_finetuned(ft_model, dataset_subset, domain_name):\n    if dataset_subset is None:\n        print(f\"No dataset available for {domain_name}\")\n        return 0.0\n\n    correct = 0\n    total = 0\n\n    # Use custom collate\n    dataloader = DataLoader(dataset_subset, batch_size=16, shuffle=False, collate_fn=custom_collate_fn)\n\n    ft_model.eval()\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=f\"Evaluating {domain_name}\"):\n            try:\n                images = batch['image'].to(device, dtype=torch.float32)\n                labels = batch['label'].to(device)\n\n                # Forward pass\n                outputs = ft_model(images)\n                predictions = outputs.argmax(dim=-1)\n\n                correct += (predictions == labels).sum().item()\n                total += labels.size(0)\n            except Exception as e:\n                print(f\"Error processing batch in {domain_name}: {e}\")\n                continue\n\n    accuracy = correct / total if total > 0 else 0.0\n    ft_model.train()  # Set back to train mode\n    return accuracy\n\n# Fine-Tuning implementation\nclass FTCLIP(nn.Module):\n    def __init__(self, clip_model, num_classes, text_features):\n        super().__init__()\n        self.visual = clip_model.visual.float()  # Ensure float32, make trainable\n        self.num_classes = num_classes\n\n        # Get the dimension of visual features\n        with torch.no_grad():\n            dummy_input = torch.randn(1, 3, 224, 224, dtype=torch.float32).to(device)\n            visual_dim = self.visual(dummy_input).shape[-1]\n\n        # Create classification head\n        self.classifier = nn.Linear(visual_dim, num_classes).float()\n\n        # Initialize with zero-shot text features\n        with torch.no_grad():\n            self.classifier.weight.data = text_features.clone().to(dtype=torch.float32)\n            self.classifier.bias.data.zero_()\n\n    def forward(self, x):\n        features = self.visual(x)\n        return self.classifier(features)\n\n# Check if we have training data\nif train_subset is None:\n    print(\"Error: No training data available for fine-tuning.\")\n    ft_id_accuracy = id_accuracy\n    ft_ood_accuracies = ood_accuracies.copy()\n    print(\"Using zero-shot results as fine-tuning results (no training performed)\")\nelse:\n    # Ensure the CLIP model and text_features are in float32\n    model = model.float()\n    text_features = text_features.to(dtype=torch.float32)\n\n    # Debugging: Check dtypes\n    print(f\"text_features dtype: {text_features.dtype}\")\n\n    # Create fine-tuned model\n    ft_model = FTCLIP(model, len(class_names), text_features).to(device)\n\n    # Debugging: Check model dtypes\n    print(f\"Visual conv1 weight dtype: {ft_model.visual.conv1.weight.dtype}\")\n    print(f\"Classifier weight dtype: {ft_model.classifier.weight.dtype}\")\n\n    # Training setup\n    optimizer = optim.AdamW(ft_model.parameters(), lr=1e-5, weight_decay=0.01)\n    criterion = nn.CrossEntropyLoss()\n    num_epochs = 3  # Same as before for consistency\n\n    # Use custom collate function for DataLoader\n    train_dataloader = DataLoader(train_subset, batch_size=16, shuffle=True, collate_fn=custom_collate_fn)\n\n    print(\"Starting fine-tuning...\")\n    print(f\"Training on {len(train_subset)} samples for {num_epochs} epochs\")\n\n    # Training loop\n    ft_model.train()\n    for epoch in range(num_epochs):\n        total_loss = 0\n        correct = 0\n        total = 0\n\n        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n            try:\n                images = batch['image'].to(device, dtype=torch.float32)\n                labels = batch['label'].to(device)\n\n                optimizer.zero_grad()\n                outputs = ft_model(images)\n\n                loss = criterion(outputs, labels)\n\n                loss.backward()\n                optimizer.step()\n\n                # Statistics\n                total_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n\n            except Exception as e:\n                print(f\"Error in training batch: {e}\")\n                continue\n\n        train_acc = 100. * correct / total if total > 0 else 0\n        avg_loss = total_loss / len(train_dataloader) if len(train_dataloader) > 0 else 0\n\n        print(f\"Epoch {epoch+1}: Total Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n\n    # Evaluate fine-tuned model\n    print(\"\\nEvaluating fine-tuned model...\")\n\n    # ID accuracy\n    ft_id_accuracy = evaluate_finetuned(ft_model, real_test_subset, \"Real\")\n    print(f\"Fine-Tuned ID Accuracy (Real): {ft_id_accuracy:.4f}\")\n\n    # OOD accuracies\n    ft_ood_accuracies = {}\n    for domain in ood_domains:\n        try:\n            domain_test_split = dataset[\"test\"]\n            domain_test_subset = filter_dataset_by_domain(domain_test_split, domain, max_samples=5000)\n            if domain_test_subset is not None:\n                ft_ood_accuracy = evaluate_finetuned(ft_model, domain_test_subset, domain.capitalize())\n                ft_ood_accuracies[domain] = ft_ood_accuracy\n                print(f\"Fine-Tuned OOD Accuracy ({domain.capitalize()}): {ft_ood_accuracy:.4f}\")\n            else:\n                ft_ood_accuracies[domain] = 0.0\n        except Exception as e:\n            print(f\"Error evaluating {domain}: {e}\")\n            ft_ood_accuracies[domain] = 0.0\n\n    print(\"\\nFine-tuning completed!\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"FINE-TUNING RESULTS SUMMARY\")\nprint(\"=\"*50)\nprint(f\"Fine-Tuned ID Accuracy: {ft_id_accuracy:.4f}\")\nprint(\"Fine-Tuned OOD Accuracies:\")\nfor domain, acc in ft_ood_accuracies.items():\n    print(f\"  {domain.capitalize()}: {acc:.4f}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9b800e9a","outputId":"2aced006-1e4b-42a4-91ff-fd4a11d42746","trusted":true,"execution":{"iopub.status.busy":"2025-09-18T07:23:18.359257Z","iopub.execute_input":"2025-09-18T07:23:18.359646Z","iopub.status.idle":"2025-09-18T07:48:21.161197Z","shell.execute_reply.started":"2025-09-18T07:23:18.359627Z","shell.execute_reply":"2025-09-18T07:48:21.160463Z"}},"outputs":[{"name":"stdout","text":"text_features dtype: torch.float32\nVisual conv1 weight dtype: torch.float32\nClassifier weight dtype: torch.float32\nStarting fine-tuning...\nTraining on 10000 samples for 3 epochs\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/3: 100%|██████████| 625/625 [07:20<00:00,  1.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Total Loss: 2.0358, Train Acc: 83.77%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/3: 100%|██████████| 625/625 [07:28<00:00,  1.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Total Loss: 0.5819, Train Acc: 92.49%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/3: 100%|██████████| 625/625 [07:27<00:00,  1.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Total Loss: 0.2968, Train Acc: 95.45%\n\nEvaluating fine-tuned model...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating Real: 100%|██████████| 313/313 [01:28<00:00,  3.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fine-Tuned ID Accuracy (Real): 0.7714\nFound 5000 samples for domain quickdraw\n","output_type":"stream"},{"name":"stderr","text":"Evaluating Quickdraw: 100%|██████████| 313/313 [01:18<00:00,  3.99it/s]","output_type":"stream"},{"name":"stdout","text":"Fine-Tuned OOD Accuracy (Quickdraw): 0.0660\n\nFine-tuning completed!\n\n==================================================\nFINE-TUNING RESULTS SUMMARY\n==================================================\nFine-Tuned ID Accuracy: 0.7714\nFine-Tuned OOD Accuracies:\n  Quickdraw: 0.0660\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":2},{"id":"0359f506","cell_type":"markdown","source":"## 3. Lipsum-FT [4 pts]\n\nImplement the Lipsum-FT method from the [paper](https://openreview.net/attachment?id=2JF8mJRJ7M&name=pdf). You may use the hyperparameters from the paper. Calculate ID and OOD accuracy of this model.","metadata":{"id":"0359f506"}},{"id":"bcd025c0","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport copy\n\n# Define evaluation function for fine-tuned model (same as before)\ndef evaluate_finetuned(ft_model, dataset_subset, domain_name):\n    if dataset_subset is None:\n        print(f\"No dataset available for {domain_name}\")\n        return 0.0\n\n    correct = 0\n    total = 0\n\n    # Use custom collate\n    dataloader = DataLoader(dataset_subset, batch_size=16, shuffle=False, collate_fn=custom_collate_fn)\n\n    ft_model.eval()\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=f\"Evaluating {domain_name}\"):\n            try:\n                images = batch['image'].to(device, dtype=torch.float32)\n                labels = batch['label'].to(device)\n\n                # Forward pass\n                outputs = ft_model(images)\n                predictions = outputs.argmax(dim=-1)\n\n                correct += (predictions == labels).sum().item()\n                total += labels.size(0)\n            except Exception as e:\n                print(f\"Error processing batch in {domain_name}: {e}\")\n                continue\n\n    accuracy = correct / total if total > 0 else 0.0\n    ft_model.train()  # Set back to train mode\n    return accuracy\n\n# Lipsum-FT implementation\nclass LipsumFTCLIP(nn.Module):\n    def __init__(self, clip_model, num_classes, text_features):\n        super().__init__()\n        self.visual = clip_model.visual.float()  # Trainable visual encoder\n        self.visual0 = copy.deepcopy(clip_model.visual).float().eval()  # Frozen copy\n        self.num_classes = num_classes\n\n        # Get the dimension of visual features\n        with torch.no_grad():\n            dummy_input = torch.randn(1, 3, 224, 224, dtype=torch.float32).to(device)\n            visual_dim = self.get_features(dummy_input).shape[-1]\n\n        # Create classification head\n        self.classifier = nn.Linear(visual_dim, num_classes).float()\n\n        # Initialize with zero-shot text features\n        with torch.no_grad():\n            self.classifier.weight.data = text_features.clone().to(dtype=torch.float32)\n            self.classifier.bias.data.zero_()\n\n    def get_features(self, x):\n        return self.visual(x)\n\n    def get_features0(self, x):\n        return self.visual0(x)\n\n    def forward(self, x):\n        features = self.get_features(x)\n        return self.classifier(features)\n\n# Check if we have training data for Lipsum-FT\nif train_subset is None:\n    print(\"Error: No training data available for Lipsum-FT.\")\n    lipsum_id_accuracy = id_accuracy\n    lipsum_ood_accuracies = ood_accuracies.copy()\n    results[\"lipsum_ft\"] = {\"id\": lipsum_id_accuracy, \"ood\": lipsum_ood_accuracies}\n    print(\"Using zero-shot results as Lipsum-FT results (no training performed)\")\nelse:\n    # Ensure the CLIP model and text_features are in float32\n    model = model.float()\n    text_features = text_features.to(dtype=torch.float32)\n\n    # Debugging: Check text_features dtype\n    print(f\"text_features dtype: {text_features.dtype}\")\n\n    # Create Lipsum-FT model\n    lipsum_model = LipsumFTCLIP(model, len(class_names), text_features).to(device)\n\n    # Debugging: Check model dtypes\n    print(f\"Visual conv1 weight dtype: {lipsum_model.visual.conv1.weight.dtype}\")\n    print(f\"Classifier weight dtype: {lipsum_model.classifier.weight.dtype}\")\n\n    # Training setup\n    optimizer_lipsum = optim.AdamW(lipsum_model.parameters(), lr=1e-5, weight_decay=0.01)\n    criterion = nn.CrossEntropyLoss()\n    num_epochs = 3\n    lambda_reg = 1.0  # Assumed value based on similar methods; adjust if needed\n    M = 80  # Number of random texts\n    L = 8  # Text length\n    context_length = 77\n    sot_token = 49406\n    eot_token = 49407\n    vocab_size = 49408  # CLIP vocab size\n\n    train_dataloader = DataLoader(train_subset, batch_size=16, shuffle=True, collate_fn=custom_collate_fn)\n\n    print(\"Starting Lipsum-FT training...\")\n    print(f\"Training on {len(train_subset)} samples for {num_epochs} epochs\")\n    print(f\"Lipsum hyperparameters: M={M}, L={L}, lambda_reg={lambda_reg}\")\n\n    # Training loop\n    lipsum_model.train()\n    for epoch in range(num_epochs):\n        total_loss = 0\n        correct = 0\n        total = 0\n\n        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n            try:\n                images = batch['image'].to(device, dtype=torch.float32)\n                labels = batch['label'].to(device)\n\n                optimizer_lipsum.zero_grad()\n\n                # Forward pass\n                outputs = lipsum_model(images)\n\n                # CE loss\n                ce_loss = criterion(outputs, labels)\n\n                # Generate random tokens\n                random_tokens = torch.zeros((M, context_length), dtype=torch.long, device=device)\n                for i in range(M):\n                    random_tokens[i, 0] = sot_token\n                    random_tokens[i, 1:1+L] = torch.randint(0, vocab_size - 2, (L,), device=device)  # Avoid SOT/EOT\n                    random_tokens[i, 1+L] = eot_token\n\n                # Get random text features (without normalization)\n                with torch.no_grad():\n                    random_text_feats = model.encode_text(random_tokens)\n\n                # Get features (without normalization)\n                features = lipsum_model.get_features(images)\n                features0 = lipsum_model.get_features0(images)\n\n                # Compute v and v0\n                v = features @ random_text_feats.T\n                v0 = features0 @ random_text_feats.T\n\n                # Regularization term\n                reg = ((v - v0) ** 2).sum(dim=1) / (2 * M)\n                reg = reg.mean()  # Average over batch\n\n                # Total loss\n                loss = ce_loss + lambda_reg * reg\n\n                loss.backward()\n                optimizer_lipsum.step()\n\n                # Statistics\n                total_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n\n            except Exception as e:\n                print(f\"Error in Lipsum training batch: {e}\")\n                continue\n\n        train_acc = 100. * correct / total if total > 0 else 0\n        avg_loss = total_loss / len(train_dataloader) if len(train_dataloader) > 0 else 0\n\n        print(f\"Epoch {epoch+1}: Total Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n\n    # Evaluate Lipsum-FT model\n    print(\"\\nEvaluating Lipsum-FT model...\")\n\n    # ID accuracy\n    lipsum_id_accuracy = evaluate_finetuned(lipsum_model, real_test_subset, \"Real\")\n    print(f\"Lipsum-FT ID Accuracy (Real): {lipsum_id_accuracy:.4f}\")\n\n    # OOD accuracies\n    lipsum_ood_accuracies = {}\n    for domain in ood_domains:\n        try:\n            domain_test_split = dataset[\"test\"]\n            if domain_test_split is not None:\n                domain_test_subset = filter_dataset_by_domain(domain_test_split, domain, max_samples=5000)\n                lipsum_ood_accuracy = evaluate_finetuned(lipsum_model, domain_test_subset, domain.capitalize())\n                lipsum_ood_accuracies[domain] = lipsum_ood_accuracy\n                print(f\"Lipsum-FT OOD Accuracy ({domain.capitalize()}): {lipsum_ood_accuracy:.4f}\")\n            else:\n                lipsum_ood_accuracies[domain] = 0.0\n        except Exception as e:\n            print(f\"Error evaluating {domain}: {e}\")\n            lipsum_ood_accuracies[domain] = 0.0\n\n    print(\"\\nLipsum-FT training completed!\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"LIPSUM-FT RESULTS SUMMARY\")\nprint(\"=\"*50)\nprint(f\"Lipsum-FT ID Accuracy: {lipsum_id_accuracy:.4f}\")\nprint(\"Lipsum-FT OOD Accuracies:\")\nfor domain, acc in lipsum_ood_accuracies.items():\n    print(f\"  {domain.capitalize()}: {acc:.4f}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bcd025c0","outputId":"af4283cd-8e9c-403c-8015-0550df9def1d","trusted":true,"execution":{"iopub.status.busy":"2025-09-18T07:48:21.162108Z","iopub.execute_input":"2025-09-18T07:48:21.162373Z","iopub.status.idle":"2025-09-18T09:00:50.090433Z","shell.execute_reply.started":"2025-09-18T07:48:21.162356Z","shell.execute_reply":"2025-09-18T09:00:50.089746Z"}},"outputs":[{"name":"stdout","text":"text_features dtype: torch.float32\nVisual conv1 weight dtype: torch.float32\nClassifier weight dtype: torch.float32\nStarting Lipsum-FT training...\nTraining on 10000 samples for 3 epochs\nLipsum hyperparameters: M=80, L=8, lambda_reg=1.0\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/3: 100%|██████████| 625/625 [23:39<00:00,  2.27s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Total Loss: 0.8013, Train Acc: 97.07%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/3: 100%|██████████| 625/625 [22:41<00:00,  2.18s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Total Loss: 0.7166, Train Acc: 96.73%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/3: 100%|██████████| 625/625 [23:12<00:00,  2.23s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Total Loss: 0.5655, Train Acc: 96.96%\n\nEvaluating Lipsum-FT model...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating Real: 100%|██████████| 313/313 [01:33<00:00,  3.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Lipsum-FT ID Accuracy (Real): 0.7846\nFound 5000 samples for domain quickdraw\n","output_type":"stream"},{"name":"stderr","text":"Evaluating Quickdraw: 100%|██████████| 313/313 [01:22<00:00,  3.79it/s]","output_type":"stream"},{"name":"stdout","text":"Lipsum-FT OOD Accuracy (Quickdraw): 0.0588\n\nLipsum-FT training completed!\n\n==================================================\nLIPSUM-FT RESULTS SUMMARY\n==================================================\nLipsum-FT ID Accuracy: 0.7846\nLipsum-FT OOD Accuracies:\n  Quickdraw: 0.0588\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":3},{"id":"c748876c","cell_type":"markdown","source":"## 4. WiSE-FT [1.5 pts]\n\nCreate a [weight-space ensemble](https://arxiv.org/pdf/2109.01903) for an ordinary fine-tuned model. Calculate ID and OOD accuracy of this model.","metadata":{"id":"c748876c"}},{"id":"2698d628","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport copy\n\n# Define evaluation function for fine-tuned model (same as before)\ndef evaluate_finetuned(ft_model, dataset_subset, domain_name):\n    if dataset_subset is None:\n        print(f\"No dataset available for {domain_name}\")\n        return 0.0\n\n    correct = 0\n    total = 0\n\n    # Use custom collate\n    dataloader = DataLoader(dataset_subset, batch_size=16, shuffle=False, collate_fn=custom_collate_fn)\n\n    ft_model.eval()\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=f\"Evaluating {domain_name}\"):\n            try:\n                images = batch['image'].to(device, dtype=torch.float32)\n                labels = batch['label'].to(device)\n\n                # Forward pass\n                outputs = ft_model(images)\n                predictions = outputs.argmax(dim=-1)\n\n                correct += (predictions == labels).sum().item()\n                total += labels.size(0)\n            except Exception as e:\n                print(f\"Error processing batch in {domain_name}: {e}\")\n                continue\n\n    accuracy = correct / total if total > 0 else 0.0\n    return accuracy  # No need to set train mode as we don't train here\n\n# Assume ft_model is the fine-tuned model from the fine-tuning section\n# Create zero-shot model using the same architecture\nzs_model = FTCLIP(model, len(class_names), text_features.to(dtype=torch.float32)).to(device)\n\n# Create WiSE-FT model by weight interpolation\nalpha = 0.5  # As per general recommendation in the paper\n\nwise_model = FTCLIP(model, len(class_names), text_features.to(dtype=torch.float32)).to(device)\n\n# Interpolate weights\nstate_dict_ft = ft_model.state_dict()\nstate_dict_zs = zs_model.state_dict()\nstate_dict_wise = {}\n\nfor key in state_dict_ft.keys():\n    state_dict_wise[key] = alpha * state_dict_ft[key] + (1 - alpha) * state_dict_zs[key]\n\nwise_model.load_state_dict(state_dict_wise)\n\nprint(f\"Created WiSE-FT model with alpha={alpha}\")\n\n# Evaluate WiSE-FT model\nprint(\"\\nEvaluating WiSE-FT model...\")\n\n# ID accuracy\nwise_id_accuracy = evaluate_finetuned(wise_model, real_test_subset, \"Real\")\nprint(f\"WiSE-FT ID Accuracy (Real): {wise_id_accuracy:.4f}\")\n\n# OOD accuracies\nwise_ood_accuracies = {}\nfor domain in ood_domains:\n    try:\n        domain_test_split = dataset[\"test\"]\n        if domain_test_split is not None:\n            domain_test_subset = filter_dataset_by_domain(domain_test_split, domain, max_samples=5000)\n            wise_ood_accuracy = evaluate_finetuned(wise_model, domain_test_subset, domain.capitalize())\n            wise_ood_accuracies[domain] = wise_ood_accuracy\n            print(f\"WiSE-FT OOD Accuracy ({domain.capitalize()}): {wise_ood_accuracy:.4f}\")\n        else:\n            wise_ood_accuracies[domain] = 0.0\n    except Exception as e:\n        print(f\"Error evaluating {domain}: {e}\")\n        wise_ood_accuracies[domain] = 0.0\n\nprint(\"\\nWiSE-FT evaluation completed!\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"WiSE-FT RESULTS SUMMARY\")\nprint(\"=\"*50)\nprint(f\"WiSE-FT ID Accuracy: {wise_id_accuracy:.4f}\")\nprint(\"WiSE-FT OOD Accuracies:\")\nfor domain, acc in wise_ood_accuracies.items():\n    print(f\"  {domain.capitalize()}: {acc:.4f}\")","metadata":{"id":"2698d628","trusted":true,"execution":{"iopub.status.busy":"2025-09-18T09:00:50.091295Z","iopub.execute_input":"2025-09-18T09:00:50.091596Z","iopub.status.idle":"2025-09-18T09:03:46.412455Z","shell.execute_reply.started":"2025-09-18T09:00:50.091575Z","shell.execute_reply":"2025-09-18T09:03:46.411721Z"}},"outputs":[{"name":"stdout","text":"Created WiSE-FT model with alpha=0.5\n\nEvaluating WiSE-FT model...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating Real: 100%|██████████| 313/313 [01:33<00:00,  3.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"WiSE-FT ID Accuracy (Real): 0.7836\nFound 5000 samples for domain quickdraw\n","output_type":"stream"},{"name":"stderr","text":"Evaluating Quickdraw: 100%|██████████| 313/313 [01:22<00:00,  3.78it/s]","output_type":"stream"},{"name":"stdout","text":"WiSE-FT OOD Accuracy (Quickdraw): 0.0590\n\nWiSE-FT evaluation completed!\n\n==================================================\nWiSE-FT RESULTS SUMMARY\n==================================================\nWiSE-FT ID Accuracy: 0.7836\nWiSE-FT OOD Accuracies:\n  Quickdraw: 0.0590\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4},{"id":"0554d8ec","cell_type":"markdown","source":"## 5. WiSE for Lipsum-FT [1.5 pts]\n\nCreate a [weight-space ensemble](https://arxiv.org/pdf/2109.01903) for Lipsum-FT model. Calculate ID and OOD accuracy of this model.","metadata":{"id":"0554d8ec"}},{"id":"4a9cb99b","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport copy\n\n# Define evaluation function for fine-tuned model (same as before)\ndef evaluate_finetuned(ft_model, dataset_subset, domain_name):\n    if dataset_subset is None:\n        print(f\"No dataset available for {domain_name}\")\n        return 0.0\n\n    correct = 0\n    total = 0\n\n    # Use custom collate\n    dataloader = DataLoader(dataset_subset, batch_size=16, shuffle=False, collate_fn=custom_collate_fn)\n\n    ft_model.eval()\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=f\"Evaluating {domain_name}\"):\n            try:\n                images = batch['image'].to(device, dtype=torch.float32)\n                labels = batch['label'].to(device)\n\n                # Forward pass\n                outputs = ft_model(images)\n                predictions = outputs.argmax(dim=-1)\n\n                correct += (predictions == labels).sum().item()\n                total += labels.size(0)\n            except Exception as e:\n                print(f\"Error processing batch in {domain_name}: {e}\")\n                continue\n\n    accuracy = correct / total if total > 0 else 0.0\n    return accuracy  # No need to set train mode as we don't train here\n\n# Assume lipsum_model is the trained Lipsum-FT model from the previous section\n# Create zero-shot model using the same architecture as Lipsum-FT\nzs_lipsum = LipsumFTCLIP(model, len(class_names), text_features.to(dtype=torch.float32)).to(device)\n\n# Create WiSE-Lipsum model by weight interpolation\nalpha = 0.5  # As per general recommendation in the paper\n\nwise_lipsum_model = LipsumFTCLIP(model, len(class_names), text_features.to(dtype=torch.float32)).to(device)\n\n# Interpolate weights\nstate_dict_lipsum = lipsum_model.state_dict()\nstate_dict_zs = zs_lipsum.state_dict()\nstate_dict_wise = {}\n\nfor key in state_dict_lipsum.keys():\n    state_dict_wise[key] = alpha * state_dict_lipsum[key] + (1 - alpha) * state_dict_zs[key]\n\nwise_lipsum_model.load_state_dict(state_dict_wise)\n\nprint(f\"Created WiSE for Lipsum-FT model with alpha={alpha}\")\n\n# Evaluate WiSE-Lipsum model\nprint(\"\\nEvaluating WiSE for Lipsum-FT model...\")\n\n# ID accuracy\nwise_lipsum_id_accuracy = evaluate_finetuned(wise_lipsum_model, real_test_subset, \"Real\")\nprint(f\"WiSE-Lipsum ID Accuracy (Real): {wise_lipsum_id_accuracy:.4f}\")\n\n# OOD accuracies\nwise_lipsum_ood_accuracies = {}\nfor domain in ood_domains:\n    try:\n        domain_test_split = dataset[\"test\"]\n        if domain_test_split is not None:\n            domain_test_subset = filter_dataset_by_domain(domain_test_split, domain, max_samples=5000)\n            wise_lipsum_ood_accuracy = evaluate_finetuned(wise_lipsum_model, domain_test_subset, domain.capitalize())\n            wise_lipsum_ood_accuracies[domain] = wise_lipsum_ood_accuracy\n            print(f\"WiSE-Lipsum OOD Accuracy ({domain.capitalize()}): {wise_lipsum_ood_accuracy:.4f}\")\n        else:\n            wise_lipsum_ood_accuracies[domain] = 0.0\n    except Exception as e:\n        print(f\"Error evaluating {domain}: {e}\")\n        wise_lipsum_ood_accuracies[domain] = 0.0\n\nprint(\"\\nWiSE for Lipsum-FT evaluation completed!\")\nprint(\"\\n\" + \"=\"*50)\nprint(\"WiSE for Lipsum-FT RESULTS SUMMARY\")\nprint(\"=\"*50)\nprint(f\"WiSE-Lipsum ID Accuracy: {wise_lipsum_id_accuracy:.4f}\")\nprint(\"WiSE-Lipsum OOD Accuracies:\")\nfor domain, acc in wise_lipsum_ood_accuracies.items():\n    print(f\"  {domain.capitalize()}: {acc:.4f}\")","metadata":{"id":"4a9cb99b","trusted":true,"execution":{"iopub.status.busy":"2025-09-18T09:03:46.413305Z","iopub.execute_input":"2025-09-18T09:03:46.413908Z","iopub.status.idle":"2025-09-18T09:06:36.215541Z","shell.execute_reply.started":"2025-09-18T09:03:46.413888Z","shell.execute_reply":"2025-09-18T09:06:36.214906Z"}},"outputs":[{"name":"stdout","text":"Created WiSE for Lipsum-FT model with alpha=0.5\n\nEvaluating WiSE for Lipsum-FT model...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating Real: 100%|██████████| 313/313 [01:30<00:00,  3.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"WiSE-Lipsum ID Accuracy (Real): 0.7846\nFound 5000 samples for domain quickdraw\n","output_type":"stream"},{"name":"stderr","text":"Evaluating Quickdraw: 100%|██████████| 313/313 [01:19<00:00,  3.94it/s]","output_type":"stream"},{"name":"stdout","text":"WiSE-Lipsum OOD Accuracy (Quickdraw): 0.0596\n\nWiSE for Lipsum-FT evaluation completed!\n\n==================================================\nWiSE for Lipsum-FT RESULTS SUMMARY\n==================================================\nWiSE-Lipsum ID Accuracy: 0.7846\nWiSE-Lipsum OOD Accuracies:\n  Quickdraw: 0.0596\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"id":"e1823972","cell_type":"markdown","source":"## Pareto front (ID-OOD plots)\n\nNow, when all the methods are trained, put them on a single Pareto front.","metadata":{"id":"e1823972"}},{"id":"LIf35JEEjS1X","cell_type":"code","source":"results = {\n    \"zero_shot\": {\"id\": id_accuracy, \"ood\": ood_accuracies},\n    \"ft\": {\"id\": ft_id_accuracy, \"ood\": ft_ood_accuracies},\n    \"lipsum_ft\": {\"id\": lipsum_id_accuracy, \"ood\": lipsum_ood_accuracies},\n    \"wise_ft\": {\"id\": wise_id_accuracy, \"ood\": wise_ood_accuracies},\n    \"wise_lipsum_ft\": {\"id\": wise_lipsum_id_accuracy, \"ood\": wise_lipsum_ood_accuracies}\n}\n\nresults","metadata":{"id":"LIf35JEEjS1X","trusted":true,"execution":{"iopub.status.busy":"2025-09-18T09:27:12.766664Z","iopub.execute_input":"2025-09-18T09:27:12.766920Z","iopub.status.idle":"2025-09-18T09:27:12.773169Z","shell.execute_reply.started":"2025-09-18T09:27:12.766903Z","shell.execute_reply":"2025-09-18T09:27:12.772468Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"{'zero_shot': {'id': 0.717, 'ood': {'quickdraw': 0.1254}},\n 'ft': {'id': 0.7714, 'ood': {'quickdraw': 0.066}},\n 'lipsum_ft': {'id': 0.7846, 'ood': {'quickdraw': 0.0588}},\n 'wise_ft': {'id': 0.7836, 'ood': {'quickdraw': 0.059}},\n 'wise_lipsum_ft': {'id': 0.7846, 'ood': {'quickdraw': 0.0596}}}"},"metadata":{}}],"execution_count":9},{"id":"a3498396","cell_type":"code","source":"import numpy as np\nimport json\n\n# Function to compute Pareto front\ndef compute_pareto_front(points):\n    \"\"\"Compute the Pareto front from a list of (x, y) points.\n\n    Args:\n        points: List of [x, y] points where x is ID accuracy and y is average OOD accuracy.\n\n    Returns:\n        np.ndarray: Array of non-dominated points sorted by x (ID accuracy).\n    \"\"\"\n    points = np.array(points)\n    n = len(points)\n    is_pareto = np.ones(n, dtype=bool)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                # A point is dominated if another point has both higher x and y\n                if (points[j, 0] >= points[i, 0]) and (points[j, 1] >= points[i, 1]) and \\\n                   (points[j, 0] > points[i, 0] or points[j, 1] > points[i, 1]):\n                    is_pareto[i] = False\n                    break\n\n    # Sort Pareto points by x (ID accuracy) in ascending order\n    pareto_points = points[is_pareto]\n    if pareto_points.size == 0:\n        print(\"Warning: No Pareto front points found.\")\n        return np.array([])\n    \n    sorted_indices = np.argsort(pareto_points[:, 0])\n    pareto_points = pareto_points[sorted_indices]\n    return pareto_points\n\n# Extract ID and average OOD accuracies\nmethods = [\"zero_shot\", \"ft\", \"lipsum_ft\", \"wise_ft\", \"wise_lipsum_ft\"]\nmethod_names = [\"Zero-Shot\", \"Fine-Tuned\", \"Lipsum-FT\", \"WiSE-FT\", \"WiSE-Lipsum\"]\npoints = []\n\nfor method in methods:\n    if method in results:\n        id_acc = results[method][\"id\"]\n        ood_accs = results[method][\"ood\"]\n        avg_ood_acc = np.mean(list(ood_accs.values())) if ood_accs else 0.0\n        if id_acc < 0 or id_acc > 1 or avg_ood_acc < 0 or avg_ood_acc > 1:\n            print(f\"Warning: Invalid accuracies for {method} (ID: {id_acc:.4f}, Avg OOD: {avg_ood_acc:.4f}), skipping.\")\n            points.append([0.0, 0.0])\n        else:\n            points.append([id_acc, avg_ood_acc])\n    else:\n        print(f\"Warning: Results for {method} not found, skipping.\")\n        points.append([0.0, 0.0])  # Placeholder to avoid errors\n\npoints = np.array(points)\n\n# Compute Pareto front\npareto_points = compute_pareto_front(points)\n\n# Prepare data for scatter plot\nscatter_data = [\n    {\n        \"x\": float(points[i, 0]) * 100,  # Convert to percentage\n        \"y\": float(points[i, 1]) * 100,  # Convert to percentage\n        \"label\": method_names[i]\n    }\n    for i in range(len(methods)) if points[i, 0] > 0 or points[i, 1] > 0  # Skip invalid points\n]\n\n# Prepare Pareto front line data (connect Pareto points)\npareto_line_data = [\n    {\"x\": float(p[0]) * 100, \"y\": float(p[1]) * 100}\n    for p in pareto_points\n]\n\n# Generate Chart.js configuration\nchart_config = {\n    \"type\": \"scatter\",\n    \"data\": {\n        \"datasets\": [\n            {\n                \"label\": \"Models\",\n                \"data\": scatter_data,\n                \"backgroundColor\": \"rgba(54, 162, 235, 0.8)\",\n                \"borderColor\": \"rgba(54, 162, 235, 1)\",\n                \"pointRadius\": 8,\n                \"showLine\": False\n            },\n            {\n                \"label\": \"Pareto Front\",\n                \"data\": pareto_line_data,\n                \"type\": \"line\",\n                \"fill\": False,\n                \"borderColor\": \"rgba(255, 99, 132, 1)\",\n                \"backgroundColor\": \"rgba(255, 99, 132, 0.8)\",\n                \"pointRadius\": 0,\n                \"borderWidth\": 2,\n                \"tension\": 0\n            }\n        ]\n    },\n    \"options\": {\n        \"responsive\": True,\n        \"plugins\": {\n            \"title\": {\n                \"display\": True,\n                \"text\": \"Pareto Front: ID vs Average OOD Accuracy\"\n            },\n            \"tooltip\": {\n                \"callbacks\": {\n                    \"label\": \"function(context) { return context.dataset.label + ': ' + context.raw.label + ' (' + context.raw.x.toFixed(2) + '%, ' + context.raw.y.toFixed(2) + '%)'; }\"\n                }\n            }\n        },\n        \"scales\": {\n            \"x\": {\n                \"title\": {\n                    \"display\": True,\n                    \"text\": \"ID Accuracy (%)\"\n                },\n                \"min\": 0,\n                \"max\": 100\n            },\n            \"y\": {\n                \"title\": {\n                    \"display\": True,\n                    \"text\": \"Average OOD Accuracy (%)\"\n                },\n                \"min\": 0,\n                \"max\": 100\n            }\n        }\n    }\n}\n\n# Print chart configuration\nprint(\"```chartjs\")\nprint(json.dumps(chart_config, indent=2))\nprint(\"```\")\n\n# Print Pareto front points for reference\nprint(\"\\nPareto Front Points (ID Accuracy, Avg OOD Accuracy):\")\nfor i, p in enumerate(pareto_points):\n    method_idx = np.where((points[:, 0] == p[0]) & (points[:, 1] == p[1]))[0]\n    if method_idx.size > 0:\n        method_idx = method_idx[0]\n        print(f\"{method_names[method_idx]}: ({p[0]*100:.2f}%, {p[1]*100:.2f}%)\")\n    else:\n        print(f\"Point ({p[0]*100:.2f}%, {p[1]*100:.2f}%): No matching method found\")","metadata":{"id":"a3498396","trusted":true,"execution":{"iopub.status.busy":"2025-09-18T09:11:37.828473Z","iopub.execute_input":"2025-09-18T09:11:37.828758Z","iopub.status.idle":"2025-09-18T09:11:37.844593Z","shell.execute_reply.started":"2025-09-18T09:11:37.828734Z","shell.execute_reply":"2025-09-18T09:11:37.843884Z"}},"outputs":[{"name":"stdout","text":"```chartjs\n{\n  \"type\": \"scatter\",\n  \"data\": {\n    \"datasets\": [\n      {\n        \"label\": \"Models\",\n        \"data\": [\n          {\n            \"x\": 71.7,\n            \"y\": 12.540000000000001,\n            \"label\": \"Zero-Shot\"\n          },\n          {\n            \"x\": 77.14,\n            \"y\": 6.6000000000000005,\n            \"label\": \"Fine-Tuned\"\n          },\n          {\n            \"x\": 78.46,\n            \"y\": 5.88,\n            \"label\": \"Lipsum-FT\"\n          },\n          {\n            \"x\": 78.36,\n            \"y\": 5.8999999999999995,\n            \"label\": \"WiSE-FT\"\n          },\n          {\n            \"x\": 78.46,\n            \"y\": 5.96,\n            \"label\": \"WiSE-Lipsum\"\n          }\n        ],\n        \"backgroundColor\": \"rgba(54, 162, 235, 0.8)\",\n        \"borderColor\": \"rgba(54, 162, 235, 1)\",\n        \"pointRadius\": 8,\n        \"showLine\": false\n      },\n      {\n        \"label\": \"Pareto Front\",\n        \"data\": [\n          {\n            \"x\": 71.7,\n            \"y\": 12.540000000000001\n          },\n          {\n            \"x\": 77.14,\n            \"y\": 6.6000000000000005\n          },\n          {\n            \"x\": 78.46,\n            \"y\": 5.96\n          }\n        ],\n        \"type\": \"line\",\n        \"fill\": false,\n        \"borderColor\": \"rgba(255, 99, 132, 1)\",\n        \"backgroundColor\": \"rgba(255, 99, 132, 0.8)\",\n        \"pointRadius\": 0,\n        \"borderWidth\": 2,\n        \"tension\": 0\n      }\n    ]\n  },\n  \"options\": {\n    \"responsive\": true,\n    \"plugins\": {\n      \"title\": {\n        \"display\": true,\n        \"text\": \"Pareto Front: ID vs Average OOD Accuracy\"\n      },\n      \"tooltip\": {\n        \"callbacks\": {\n          \"label\": \"function(context) { return context.dataset.label + ': ' + context.raw.label + ' (' + context.raw.x.toFixed(2) + '%, ' + context.raw.y.toFixed(2) + '%)'; }\"\n        }\n      }\n    },\n    \"scales\": {\n      \"x\": {\n        \"title\": {\n          \"display\": true,\n          \"text\": \"ID Accuracy (%)\"\n        },\n        \"min\": 0,\n        \"max\": 100\n      },\n      \"y\": {\n        \"title\": {\n          \"display\": true,\n          \"text\": \"Average OOD Accuracy (%)\"\n        },\n        \"min\": 0,\n        \"max\": 100\n      }\n    }\n  }\n}\n```\n\nPareto Front Points (ID Accuracy, Avg OOD Accuracy):\nZero-Shot: (71.70%, 12.54%)\nFine-Tuned: (77.14%, 6.60%)\nWiSE-Lipsum: (78.46%, 5.96%)\n","output_type":"stream"}],"execution_count":8},{"id":"03e7d904","cell_type":"markdown","source":"## Bonus: Fine-tuning data-efficiency [2 pts]\n\nCreate a data-efficiency plot similar to Figure 6 from the [CLIP paper](https://arxiv.org/pdf/2103.00020) for the considered fine-tuning methods (regular vs. Lipsum-FT). On the horizontal axis you will have the number of fine-tuning samples per class (in logarithmic scale) and on the vertical axis &mdash; ID accuracy. What conclusions can be drawn about the data-efficiency of these methods?","metadata":{"id":"03e7d904"}},{"id":"ef0b98e8","cell_type":"code","source":"","metadata":{"id":"ef0b98e8","trusted":true},"outputs":[],"execution_count":null}]}