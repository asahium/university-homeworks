{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98eefa05",
   "metadata": {},
   "source": [
    "# LSDL CUB, Homework 1. Robust fine-tuning of CLIP [10 pts]\n",
    "\n",
    "Your main goal in this home assignment is to implement the [Lipsum-FT](https://openreview.net/attachment?id=2JF8mJRJ7M&name=pdf) method for robust fine-tuning of CLIP.\n",
    "\n",
    "Rules for the assignment:\n",
    "\n",
    "- We will be using the same dataset, [DomainNet](https://ai.bu.edu/M3SDA/) (also can be found [here](https://huggingface.co/datasets/wltjr1007/DomainNet)), as in the original paper. Use the **Real** domain as in-distribution (ID) data and the rest of domains as out-of-distribution (OOD) data. Use the **Real** train split for training and test splits for evaluation on all of the domains.\n",
    "\n",
    "- If training takes too much time, you may select a subset (i.e., 50% or 33%) of training data instead of the full split.\n",
    "\n",
    "- `ViT-B/16` backbone is recommended.\n",
    "\n",
    "- In order to **pass the assignment**, you need to plot a Pareto front (i.e., ID-OOD plot like Figure 1 from this [paper](https://arxiv.org/pdf/2109.01903)). We will be using 5 OOD domains, so you need to plot 5 Pareto fronts, one for each distribution shift.\n",
    "\n",
    "- You may use any code from the [seminar](https://github.com/isadrtdinov/lsdl-cub/tree/2025/week01-finetune/seminar). Also, you may code your training pipelines either in pure PyTorch or combinine it with [huggingface](https://huggingface.co/) libraries. Additionally, you may find [`clip`](https://github.com/openai/CLIP) and [`wise-ft`](https://github.com/mlfoundations/wise-ft) repos useful.\n",
    "\n",
    "- Do not use the implemention from the authors or any publicly available implementations of this method.\n",
    "\n",
    "- It will be much easier to check your assigment if you maintain a clear code structure (e.g., put different blocks of code into separate files, add necessary coments, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8568a105",
   "metadata": {},
   "source": [
    "## 1. Zero-shot model [1 pts]\n",
    "\n",
    "Create a zero-shot model on top of pre-trained CLIP and evaluate it on **Real** test set (ID accuracy) and on 5 distribution shifts: **Clipart**, **Infograph**, **Painting**, **Quickdraw**, **Sketch** (OOD accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33153ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install CLIP if not already installed\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import clip\n",
    "except ImportError:\n",
    "    print(\"Installing CLIP...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"git+https://github.com/openai/CLIP.git\"])\n",
    "    import clip\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load pre-trained CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "\n",
    "# Load DomainNet dataset\n",
    "print(\"Loading DomainNet dataset...\")\n",
    "dataset = load_dataset(\"wltjr1007/DomainNet\")\n",
    "\n",
    "# Check available splits\n",
    "print(\"Available dataset splits:\", list(dataset.keys()))\n",
    "\n",
    "# The dataset structure is different - let's check the actual structure\n",
    "sample = dataset[\"train\"][0]\n",
    "print(\"Sample from train:\", sample)\n",
    "\n",
    "# Check domain mapping\n",
    "domain_mapping = {\n",
    "    0: \"real\",\n",
    "    1: \"clipart\", \n",
    "    2: \"infograph\",\n",
    "    3: \"quickdraw\",\n",
    "    4: \"painting\",\n",
    "    5: \"sketch\"\n",
    "}\n",
    "\n",
    "# Get all unique domains in the dataset\n",
    "unique_domains = set()\n",
    "for split in ['train', 'test']:\n",
    "    sample_batch = dataset[split][:100]  # Check first 100 samples\n",
    "    for domain_id in sample_batch['domain']:\n",
    "        unique_domains.add(domain_id)\n",
    "\n",
    "print(\"Unique domain IDs found:\", sorted(unique_domains))\n",
    "\n",
    "# Map domain IDs to names\n",
    "available_domain_names = []\n",
    "for domain_id in sorted(unique_domains):\n",
    "    if domain_id in domain_mapping:\n",
    "        available_domain_names.append(domain_mapping[domain_id])\n",
    "    else:\n",
    "        available_domain_names.append(f\"domain_{domain_id}\")\n",
    "\n",
    "print(\"Available domains:\", available_domain_names)\n",
    "\n",
    "# Define domains\n",
    "id_domain = \"real\"  # domain_id = 0\n",
    "ood_domains = [d for d in available_domain_names if d != id_domain]\n",
    "\n",
    "print(f\"ID domain: {id_domain}\")\n",
    "print(f\"OOD domains: {ood_domains}\")\n",
    "\n",
    "# Get class names\n",
    "class_names = dataset[\"train\"].features[\"label\"].names\n",
    "print(f\"Number of classes: {len(class_names)}\")\n",
    "print(f\"First 10 classes: {class_names[:10]}\")\n",
    "\n",
    "# Create text prompts for zero-shot classification\n",
    "text_prompts = [f\"a photo of a {class_name}\" for class_name in class_names]\n",
    "text_inputs = clip.tokenize(text_prompts).to(device)\n",
    "\n",
    "# Get text features\n",
    "print(\"Encoding text prompts...\")\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "def filter_dataset_by_domain(dataset_split, domain_name, max_samples=None):\n",
    "    \"\"\"Filter dataset by domain and optionally limit number of samples\"\"\"\n",
    "    # Get domain ID from name\n",
    "    domain_id = None\n",
    "    for did, dname in domain_mapping.items():\n",
    "        if dname == domain_name:\n",
    "            domain_id = did\n",
    "            break\n",
    "    \n",
    "    if domain_id is None:\n",
    "        print(f\"Domain {domain_name} not found in mapping\")\n",
    "        return None\n",
    "    \n",
    "    # Filter by domain\n",
    "    filtered_indices = []\n",
    "    for i, domain in enumerate(dataset_split['domain']):\n",
    "        if domain == domain_id:\n",
    "            filtered_indices.append(i)\n",
    "            if max_samples and len(filtered_indices) >= max_samples:\n",
    "                break\n",
    "    \n",
    "    print(f\"Found {len(filtered_indices)} samples for domain {domain_name}\")\n",
    "    \n",
    "    if len(filtered_indices) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Create subset\n",
    "    return Subset(dataset_split, filtered_indices)\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle PIL images\"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for item in batch:\n",
    "        # Convert PIL image to tensor using preprocess\n",
    "        image = preprocess(item['image'].convert('RGB'))\n",
    "        images.append(image)\n",
    "        labels.append(item['label'])\n",
    "    \n",
    "    return {\n",
    "        'image': torch.stack(images),\n",
    "        'label': torch.tensor(labels)\n",
    "    }\n",
    "\n",
    "def evaluate_zero_shot(dataset_subset, domain_name):\n",
    "    \"\"\"Evaluate zero-shot CLIP on a dataset subset\"\"\"\n",
    "    if dataset_subset is None:\n",
    "        print(f\"No dataset available for {domain_name}\")\n",
    "        return 0.0\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Create dataloader with custom collate function\n",
    "    dataloader = DataLoader(dataset_subset, batch_size=16, shuffle=False, collate_fn=custom_collate_fn)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=f\"Evaluating {domain_name}\"):\n",
    "            try:\n",
    "                images = batch['image'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                # Get image features\n",
    "                image_features = model.encode_image(images)\n",
    "                image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "                \n",
    "                # Calculate similarities and predictions\n",
    "                similarities = (image_features @ text_features.T)\n",
    "                predictions = similarities.argmax(dim=-1)\n",
    "                \n",
    "                correct += (predictions == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch in {domain_name}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate on ID (Real) domain\n",
    "print(f\"\\nEvaluating on ID domain ({id_domain})...\")\n",
    "real_test_subset = filter_dataset_by_domain(dataset[\"test\"], id_domain, max_samples=5000)  # Limit for speed\n",
    "\n",
    "if real_test_subset is not None:\n",
    "    id_accuracy = evaluate_zero_shot(real_test_subset, \"Real\")\n",
    "    print(f\"ID Accuracy (Real): {id_accuracy:.4f}\")\n",
    "else:\n",
    "    print(\"Could not find Real domain test data\")\n",
    "    id_accuracy = 0.0\n",
    "\n",
    "# Evaluate on OOD domains\n",
    "ood_accuracies = {}\n",
    "print(f\"\\nEvaluating on OOD domains...\")\n",
    "for domain in ood_domains:\n",
    "    try:\n",
    "        domain_test_subset = filter_dataset_by_domain(dataset[\"test\"], domain, max_samples=5000)  # Limit for speed\n",
    "        if domain_test_subset is not None:\n",
    "            ood_accuracy = evaluate_zero_shot(domain_test_subset, domain.capitalize())\n",
    "            ood_accuracies[domain] = ood_accuracy\n",
    "            print(f\"OOD Accuracy ({domain.capitalize()}): {ood_accuracy:.4f}\")\n",
    "        else:\n",
    "            print(f\"Could not find test data for domain: {domain}\")\n",
    "            ood_accuracies[domain] = 0.0\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {domain}: {e}\")\n",
    "        ood_accuracies[domain] = 0.0\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ZERO-SHOT RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"ID Accuracy (Real): {id_accuracy:.4f}\")\n",
    "print(\"OOD Accuracies:\")\n",
    "for domain, acc in ood_accuracies.items():\n",
    "    print(f\"  {domain.capitalize()}: {acc:.4f}\")\n",
    "\n",
    "avg_ood_accuracy = np.mean(list(ood_accuracies.values())) if ood_accuracies else 0.0\n",
    "print(f\"Average OOD Accuracy: {avg_ood_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nZero-shot evaluation completed!\")\n",
    "\n",
    "# Store variables for next cells\n",
    "real_train_subset = filter_dataset_by_domain(dataset[\"train\"], id_domain, max_samples=10000)  # Limit training data\n",
    "if real_train_subset is not None:\n",
    "    print(f\"Training subset size: {len(real_train_subset)}\")\n",
    "    \n",
    "    # Store additional variables needed for training\n",
    "    train_subset = real_train_subset\n",
    "    real_test_subset = real_test_subset\n",
    "else:\n",
    "    train_subset = None\n",
    "    print(\"Warning: Could not find Real domain training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59735c0f",
   "metadata": {},
   "source": [
    "## 2. Regular fine-tuning [2 pts]\n",
    "\n",
    "Now, fine-tune the whole image encoder on the **Real** train split. Use the zero-shot classification head as an initialization for the last linear layer. Calculate ID and OOD accuracy of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b800e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Create a fine-tunable model by replacing the final layer\n",
    "class FineTunedCLIP(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes):\n",
    "        super().__init__()\n",
    "        self.visual = clip_model.visual\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Get the dimension of visual features\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "            visual_dim = self.visual(dummy_input).shape[-1]\n",
    "        \n",
    "        # Create classification head initialized with text features\n",
    "        self.classifier = nn.Linear(visual_dim, num_classes)\n",
    "        \n",
    "        # Initialize with zero-shot text features (transposed)\n",
    "        with torch.no_grad():\n",
    "            self.classifier.weight.data = text_features.clone()\n",
    "            self.classifier.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.visual(x)\n",
    "        return self.classifier(features)\n",
    "\n",
    "# Check if we have training data\n",
    "if train_subset is None:\n",
    "    print(\"Error: No training data available. Please check the dataset structure.\")\n",
    "    # Create dummy results to continue\n",
    "    ft_id_accuracy = id_accuracy\n",
    "    ft_ood_accuracies = ood_accuracies.copy()\n",
    "    results = {\n",
    "        \"zero_shot\": {\"id\": id_accuracy, \"ood\": ood_accuracies},\n",
    "        \"fine_tuned\": {\"id\": ft_id_accuracy, \"ood\": ft_ood_accuracies}\n",
    "    }\n",
    "    print(\"Using zero-shot results as fine-tuned results (no training performed)\")\n",
    "else:\n",
    "    # Create fine-tuned model\n",
    "    ft_model = FineTunedCLIP(model, len(class_names)).to(device)\n",
    "\n",
    "    # Training setup\n",
    "    optimizer = optim.AdamW(ft_model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    num_epochs = 3  # Reduced for faster training\n",
    "\n",
    "    # Prepare training data with custom collate function\n",
    "    train_dataloader = DataLoader(train_subset, batch_size=16, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "    print(\"Starting regular fine-tuning...\")\n",
    "    print(f\"Training on {len(train_subset)} samples for {num_epochs} epochs\")\n",
    "\n",
    "    # Training loop\n",
    "    ft_model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            try:\n",
    "                images = batch['image'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                outputs = ft_model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "            except Exception as e:\n",
    "                print(f\"Error in training batch: {e}\")\n",
    "                continue\n",
    "        \n",
    "        train_acc = 100. * correct / total if total > 0 else 0\n",
    "        avg_loss = total_loss / len(train_dataloader) if len(train_dataloader) > 0 else 0\n",
    "        print(f\"Epoch {epoch+1}: Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "\n",
    "    def evaluate_finetuned(model, dataset_subset, domain_name):\n",
    "        \"\"\"Evaluate fine-tuned model on a dataset subset\"\"\"\n",
    "        if dataset_subset is None:\n",
    "            return 0.0\n",
    "            \n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        dataloader = DataLoader(dataset_subset, batch_size=16, shuffle=False, collate_fn=custom_collate_fn)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=f\"Evaluating {domain_name}\"):\n",
    "                try:\n",
    "                    images = batch['image'].to(device)\n",
    "                    labels = batch['label'].to(device)\n",
    "                    \n",
    "                    outputs = model(images)\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    \n",
    "                    correct += predicted.eq(labels).sum().item()\n",
    "                    total += labels.size(0)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in evaluation batch: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        accuracy = correct / total if total > 0 else 0.0\n",
    "        return accuracy\n",
    "\n",
    "    # Evaluate fine-tuned model\n",
    "    print(\"\\nEvaluating fine-tuned model...\")\n",
    "\n",
    "    # ID accuracy\n",
    "    ft_id_accuracy = evaluate_finetuned(ft_model, real_test_subset, \"Real\")\n",
    "    print(f\"Fine-tuned ID Accuracy (Real): {ft_id_accuracy:.4f}\")\n",
    "\n",
    "    # OOD accuracies\n",
    "    ft_ood_accuracies = {}\n",
    "    for domain in ood_domains:\n",
    "        try:\n",
    "            domain_test_subset = filter_dataset_by_domain(dataset[\"test\"], domain, max_samples=5000)\n",
    "            if domain_test_subset is not None:\n",
    "                ft_ood_accuracy = evaluate_finetuned(ft_model, domain_test_subset, domain.capitalize())\n",
    "                ft_ood_accuracies[domain] = ft_ood_accuracy\n",
    "                print(f\"Fine-tuned OOD Accuracy ({domain.capitalize()}): {ft_ood_accuracy:.4f}\")\n",
    "            else:\n",
    "                ft_ood_accuracies[domain] = 0.0\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {domain}: {e}\")\n",
    "            ft_ood_accuracies[domain] = 0.0\n",
    "\n",
    "    print(\"\\nRegular fine-tuning completed!\")\n",
    "\n",
    "    # Store results for later comparison\n",
    "    results = {\n",
    "        \"zero_shot\": {\"id\": id_accuracy, \"ood\": ood_accuracies},\n",
    "        \"fine_tuned\": {\"id\": ft_id_accuracy, \"ood\": ft_ood_accuracies}\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINE-TUNING RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Fine-tuned ID Accuracy: {ft_id_accuracy:.4f}\")\n",
    "print(\"Fine-tuned OOD Accuracies:\")\n",
    "for domain, acc in ft_ood_accuracies.items():\n",
    "    print(f\"  {domain.capitalize()}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0359f506",
   "metadata": {},
   "source": [
    "## 3. Lipsum-FT [4 pts]\n",
    "\n",
    "Implement the Lipsum-FT method from the [paper](https://openreview.net/attachment?id=2JF8mJRJ7M&name=pdf). You may use the hyperparameters from the paper. Calculate ID and OOD accuracy of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd025c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Lipsum-FT implementation\n",
    "class LipsumFTCLIP(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes):\n",
    "        super().__init__()\n",
    "        self.visual = clip_model.visual\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Get the dimension of visual features\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "            visual_dim = self.visual(dummy_input).shape[-1]\n",
    "        \n",
    "        # Create classification head initialized with text features\n",
    "        self.classifier = nn.Linear(visual_dim, num_classes)\n",
    "        \n",
    "        # Initialize with zero-shot text features\n",
    "        with torch.no_grad():\n",
    "            self.classifier.weight.data = text_features.clone()\n",
    "            self.classifier.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.visual(x)\n",
    "        return self.classifier(features)\n",
    "\n",
    "def generate_lipsum_data(real_features, batch_size, lipsum_ratio=0.5):\n",
    "    \"\"\"Generate Lipsum (synthetic) data by mixing real features\"\"\"\n",
    "    num_lipsum = int(batch_size * lipsum_ratio)\n",
    "    num_real = batch_size - num_lipsum\n",
    "    \n",
    "    # Create mixed features\n",
    "    mixed_features = real_features.clone()\n",
    "    lipsum_mask = torch.zeros(batch_size, dtype=torch.bool, device=real_features.device)\n",
    "    \n",
    "    if num_lipsum > 0 and num_real >= 2:\n",
    "        # Generate Lipsum samples by mixing random pairs of real samples\n",
    "        for i in range(num_real, batch_size):\n",
    "            # Select two random real samples to mix\n",
    "            idx1, idx2 = random.sample(range(num_real), 2)\n",
    "            \n",
    "            # Mix with random weights\n",
    "            alpha = random.uniform(0.3, 0.7)\n",
    "            mixed_features[i] = alpha * real_features[idx1] + (1 - alpha) * real_features[idx2]\n",
    "            lipsum_mask[i] = True\n",
    "    \n",
    "    return mixed_features, lipsum_mask\n",
    "\n",
    "# Check if we have training data for Lipsum-FT\n",
    "if train_subset is None:\n",
    "    print(\"Error: No training data available for Lipsum-FT.\")\n",
    "    # Use zero-shot results as placeholder\n",
    "    lipsum_id_accuracy = id_accuracy\n",
    "    lipsum_ood_accuracies = ood_accuracies.copy()\n",
    "    results[\"lipsum_ft\"] = {\"id\": lipsum_id_accuracy, \"ood\": lipsum_ood_accuracies}\n",
    "    print(\"Using zero-shot results as Lipsum-FT results (no training performed)\")\n",
    "else:\n",
    "    # Create Lipsum-FT model\n",
    "    lipsum_model = LipsumFTCLIP(model, len(class_names)).to(device)\n",
    "\n",
    "    # Training setup\n",
    "    optimizer_lipsum = optim.AdamW(lipsum_model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "    criterion_lipsum = nn.CrossEntropyLoss(reduction='none')\n",
    "    num_epochs = 3  # Reduced for faster training\n",
    "    lipsum_ratio = 0.5\n",
    "    lipsum_weight = 1.0\n",
    "\n",
    "    # Use the same training dataloader with custom collate function\n",
    "    train_dataloader = DataLoader(train_subset, batch_size=16, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "    print(\"Starting Lipsum-FT training...\")\n",
    "    print(f\"Training on {len(train_subset)} samples for {num_epochs} epochs\")\n",
    "    print(f\"Lipsum ratio: {lipsum_ratio}, Lipsum weight: {lipsum_weight}\")\n",
    "\n",
    "    # Training loop\n",
    "    lipsum_model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        real_loss_sum = 0\n",
    "        lipsum_loss_sum = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            try:\n",
    "                images = batch['image'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                optimizer_lipsum.zero_grad()\n",
    "                \n",
    "                # Extract features\n",
    "                with torch.no_grad():\n",
    "                    features = lipsum_model.visual(images)\n",
    "                \n",
    "                # Generate mixed features with Lipsum data\n",
    "                mixed_features, lipsum_mask = generate_lipsum_data(features, features.size(0), lipsum_ratio)\n",
    "                \n",
    "                # Forward pass through classifier\n",
    "                outputs = lipsum_model.classifier(mixed_features)\n",
    "                \n",
    "                # Calculate loss\n",
    "                losses = criterion_lipsum(outputs, labels)\n",
    "                \n",
    "                # Separate real and Lipsum losses\n",
    "                real_mask = ~lipsum_mask\n",
    "                real_loss = losses[real_mask].mean() if real_mask.any() else torch.tensor(0.0, device=device)\n",
    "                lipsum_loss = losses[lipsum_mask].mean() if lipsum_mask.any() else torch.tensor(0.0, device=device)\n",
    "                \n",
    "                # Combined loss\n",
    "                total_batch_loss = real_loss + lipsum_weight * lipsum_loss\n",
    "                \n",
    "                total_batch_loss.backward()\n",
    "                optimizer_lipsum.step()\n",
    "                \n",
    "                # Statistics\n",
    "                total_loss += total_batch_loss.item()\n",
    "                if isinstance(real_loss, torch.Tensor) and real_loss.item() > 0:\n",
    "                    real_loss_sum += real_loss.item()\n",
    "                if isinstance(lipsum_loss, torch.Tensor) and lipsum_loss.item() > 0:\n",
    "                    lipsum_loss_sum += lipsum_loss.item()\n",
    "                \n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                # Only count accuracy on real samples\n",
    "                if real_mask.any():\n",
    "                    correct += predicted[real_mask].eq(labels[real_mask]).sum().item()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error in Lipsum training batch: {e}\")\n",
    "                continue\n",
    "        \n",
    "        real_samples = max(1, total - int(total * lipsum_ratio))\n",
    "        train_acc = 100. * correct / real_samples\n",
    "        avg_loss = total_loss / len(train_dataloader) if len(train_dataloader) > 0 else 0\n",
    "        avg_real_loss = real_loss_sum / len(train_dataloader) if len(train_dataloader) > 0 else 0\n",
    "        avg_lipsum_loss = lipsum_loss_sum / len(train_dataloader) if len(train_dataloader) > 0 else 0\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Total Loss: {avg_loss:.4f}, Real Loss: {avg_real_loss:.4f}, \"\n",
    "              f\"Lipsum Loss: {avg_lipsum_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "\n",
    "    # Evaluate Lipsum-FT model\n",
    "    print(\"\\nEvaluating Lipsum-FT model...\")\n",
    "\n",
    "    # ID accuracy\n",
    "    lipsum_id_accuracy = evaluate_finetuned(lipsum_model, real_test_subset, \"Real\")\n",
    "    print(f\"Lipsum-FT ID Accuracy (Real): {lipsum_id_accuracy:.4f}\")\n",
    "\n",
    "    # OOD accuracies\n",
    "    lipsum_ood_accuracies = {}\n",
    "    for domain in ood_domains:\n",
    "        try:\n",
    "            domain_test_subset = filter_dataset_by_domain(dataset[\"test\"], domain, max_samples=5000)\n",
    "            if domain_test_subset is not None:\n",
    "                lipsum_ood_accuracy = evaluate_finetuned(lipsum_model, domain_test_subset, domain.capitalize())\n",
    "                lipsum_ood_accuracies[domain] = lipsum_ood_accuracy\n",
    "                print(f\"Lipsum-FT OOD Accuracy ({domain.capitalize()}): {lipsum_ood_accuracy:.4f}\")\n",
    "            else:\n",
    "                lipsum_ood_accuracies[domain] = 0.0\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {domain}: {e}\")\n",
    "            lipsum_ood_accuracies[domain] = 0.0\n",
    "\n",
    "    print(\"\\nLipsum-FT training completed!\")\n",
    "\n",
    "    # Update results\n",
    "    results[\"lipsum_ft\"] = {\"id\": lipsum_id_accuracy, \"ood\": lipsum_ood_accuracies}\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LIPSUM-FT RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Lipsum-FT ID Accuracy: {lipsum_id_accuracy:.4f}\")\n",
    "print(\"Lipsum-FT OOD Accuracies:\")\n",
    "for domain, acc in lipsum_ood_accuracies.items():\n",
    "    print(f\"  {domain.capitalize()}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c748876c",
   "metadata": {},
   "source": [
    "## 4. WiSE-FT [1.5 pts]\n",
    "\n",
    "Create a [weight-space ensemble](https://arxiv.org/pdf/2109.01903) for an ordinary fine-tuned model. Calculate ID and OOD accuracy of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2698d628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WiSE-FT (Weight-space ensemble) for regular fine-tuning\n",
    "\n",
    "def interpolate_models(model1, model2, alpha):\n",
    "    \"\"\"Interpolate between two models in weight space\"\"\"\n",
    "    interpolated_model = copy.deepcopy(model1)\n",
    "    \n",
    "    state_dict1 = model1.state_dict()\n",
    "    state_dict2 = model2.state_dict()\n",
    "    \n",
    "    interpolated_state_dict = {}\n",
    "    for key in state_dict1.keys():\n",
    "        if key in state_dict2:\n",
    "            interpolated_state_dict[key] = (1 - alpha) * state_dict1[key] + alpha * state_dict2[key]\n",
    "        else:\n",
    "            interpolated_state_dict[key] = state_dict1[key]\n",
    "    \n",
    "    interpolated_model.load_state_dict(interpolated_state_dict)\n",
    "    return interpolated_model\n",
    "\n",
    "# Create zero-shot model wrapper for compatibility\n",
    "class ZeroShotWrapper(nn.Module):\n",
    "    def __init__(self, clip_model, text_features):\n",
    "        super().__init__()\n",
    "        self.visual = clip_model.visual\n",
    "        self.text_features = text_features\n",
    "        \n",
    "        # Create dummy classifier for compatibility\n",
    "        visual_dim = text_features.shape[1]\n",
    "        num_classes = text_features.shape[0]\n",
    "        self.classifier = nn.Linear(visual_dim, num_classes)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.classifier.weight.data = text_features.clone()\n",
    "            self.classifier.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.visual(x)\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "        similarities = features @ self.text_features.T\n",
    "        return similarities\n",
    "\n",
    "# Create zero-shot wrapper\n",
    "zero_shot_wrapper = ZeroShotWrapper(model, text_features).to(device)\n",
    "\n",
    "# Check if we have a fine-tuned model\n",
    "if train_subset is None or 'ft_model' not in locals():\n",
    "    print(\"No fine-tuned model available. Using zero-shot results for WiSE-FT.\")\n",
    "    wise_id_accuracy = id_accuracy\n",
    "    wise_ood_accuracies = ood_accuracies.copy()\n",
    "    best_alpha = 0.0\n",
    "    results[\"wise_regular\"] = {\"id\": wise_id_accuracy, \"ood\": wise_ood_accuracies}\n",
    "else:\n",
    "    # Test different alpha values for WiSE-FT\n",
    "    alphas = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]\n",
    "    best_alpha = 0.5\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    print(\"Testing WiSE-FT with different alpha values...\")\n",
    "    for alpha in alphas:\n",
    "        try:\n",
    "            wise_model = interpolate_models(zero_shot_wrapper, ft_model, alpha)\n",
    "            accuracy = evaluate_finetuned(wise_model, real_test_subset, f\"Alpha={alpha:.1f}\")\n",
    "            print(f\"Alpha: {alpha:.1f}, ID Accuracy: {accuracy:.4f}\")\n",
    "            \n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_alpha = alpha\n",
    "        except Exception as e:\n",
    "            print(f\"Error testing alpha {alpha}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"\\nBest alpha: {best_alpha} with accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "    # Create final WiSE-FT model with best alpha\n",
    "    wise_ft_model = interpolate_models(zero_shot_wrapper, ft_model, best_alpha)\n",
    "\n",
    "    # Evaluate WiSE-FT model\n",
    "    print(f\"\\nEvaluating WiSE-FT model with alpha={best_alpha}...\")\n",
    "\n",
    "    # ID accuracy\n",
    "    wise_id_accuracy = evaluate_finetuned(wise_ft_model, real_test_subset, \"Real\")\n",
    "    print(f\"WiSE-FT ID Accuracy (Real): {wise_id_accuracy:.4f}\")\n",
    "\n",
    "    # OOD accuracies\n",
    "    wise_ood_accuracies = {}\n",
    "    for domain in ood_domains:\n",
    "        try:\n",
    "            domain_test_subset = filter_dataset_by_domain(dataset[\"test\"], domain, max_samples=5000)\n",
    "            if domain_test_subset is not None:\n",
    "                wise_ood_accuracy = evaluate_finetuned(wise_ft_model, domain_test_subset, domain.capitalize())\n",
    "                wise_ood_accuracies[domain] = wise_ood_accuracy\n",
    "                print(f\"WiSE-FT OOD Accuracy ({domain.capitalize()}): {wise_ood_accuracy:.4f}\")\n",
    "            else:\n",
    "                wise_ood_accuracies[domain] = 0.0\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {domain}: {e}\")\n",
    "            wise_ood_accuracies[domain] = 0.0\n",
    "\n",
    "    print(\"\\nWiSE-FT (Regular) completed!\")\n",
    "\n",
    "    # Update results\n",
    "    results[\"wise_regular\"] = {\"id\": wise_id_accuracy, \"ood\": wise_ood_accuracies}\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"WISE-FT (REGULAR) RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"WiSE-FT (Regular) ID Accuracy: {wise_id_accuracy:.4f}\")\n",
    "print(f\"Best alpha: {best_alpha}\")\n",
    "print(\"WiSE-FT (Regular) OOD Accuracies:\")\n",
    "for domain, acc in wise_ood_accuracies.items():\n",
    "    print(f\"  {domain.capitalize()}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0554d8ec",
   "metadata": {},
   "source": [
    "## 5. WiSE for Lipsum-FT [1.5 pts]\n",
    "\n",
    "Create a [weight-space ensemble](https://arxiv.org/pdf/2109.01903) for Lipsum-FT model. Calculate ID and OOD accuracy of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9cb99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WiSE-FT for Lipsum-FT model\n",
    "\n",
    "# Check if we have a Lipsum-FT model\n",
    "if train_subset is None or 'lipsum_model' not in locals():\n",
    "    print(\"No Lipsum-FT model available. Using zero-shot results for WiSE-FT Lipsum.\")\n",
    "    wise_lipsum_id_accuracy = id_accuracy\n",
    "    wise_lipsum_ood_accuracies = ood_accuracies.copy()\n",
    "    best_alpha_lipsum = 0.0\n",
    "    results[\"wise_lipsum\"] = {\"id\": wise_lipsum_id_accuracy, \"ood\": wise_lipsum_ood_accuracies}\n",
    "else:\n",
    "    # Test different alpha values for WiSE-FT with Lipsum\n",
    "    alphas = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]\n",
    "    best_alpha_lipsum = 0.5\n",
    "    best_accuracy_lipsum = 0.0\n",
    "\n",
    "    print(\"Testing WiSE-FT for Lipsum-FT with different alpha values...\")\n",
    "    for alpha in alphas:\n",
    "        try:\n",
    "            wise_lipsum_model = interpolate_models(zero_shot_wrapper, lipsum_model, alpha)\n",
    "            accuracy = evaluate_finetuned(wise_lipsum_model, real_test_subset, f\"Alpha={alpha:.1f}\")\n",
    "            print(f\"Alpha: {alpha:.1f}, ID Accuracy: {accuracy:.4f}\")\n",
    "            \n",
    "            if accuracy > best_accuracy_lipsum:\n",
    "                best_accuracy_lipsum = accuracy\n",
    "                best_alpha_lipsum = alpha\n",
    "        except Exception as e:\n",
    "            print(f\"Error testing alpha {alpha} for Lipsum: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"\\nBest alpha for Lipsum: {best_alpha_lipsum} with accuracy: {best_accuracy_lipsum:.4f}\")\n",
    "\n",
    "    # Create final WiSE-FT Lipsum model with best alpha\n",
    "    wise_lipsum_ft_model = interpolate_models(zero_shot_wrapper, lipsum_model, best_alpha_lipsum)\n",
    "\n",
    "    # Evaluate WiSE-FT Lipsum model\n",
    "    print(f\"\\nEvaluating WiSE-FT Lipsum model with alpha={best_alpha_lipsum}...\")\n",
    "\n",
    "    # ID accuracy\n",
    "    wise_lipsum_id_accuracy = evaluate_finetuned(wise_lipsum_ft_model, real_test_subset, \"Real\")\n",
    "    print(f\"WiSE-FT Lipsum ID Accuracy (Real): {wise_lipsum_id_accuracy:.4f}\")\n",
    "\n",
    "    # OOD accuracies\n",
    "    wise_lipsum_ood_accuracies = {}\n",
    "    for domain in ood_domains:\n",
    "        try:\n",
    "            domain_test_subset = filter_dataset_by_domain(dataset[\"test\"], domain, max_samples=5000)\n",
    "            if domain_test_subset is not None:\n",
    "                wise_lipsum_ood_accuracy = evaluate_finetuned(wise_lipsum_ft_model, domain_test_subset, domain.capitalize())\n",
    "                wise_lipsum_ood_accuracies[domain] = wise_lipsum_ood_accuracy\n",
    "                print(f\"WiSE-FT Lipsum OOD Accuracy ({domain.capitalize()}): {wise_lipsum_ood_accuracy:.4f}\")\n",
    "            else:\n",
    "                wise_lipsum_ood_accuracies[domain] = 0.0\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {domain}: {e}\")\n",
    "            wise_lipsum_ood_accuracies[domain] = 0.0\n",
    "\n",
    "    print(\"\\nWiSE-FT (Lipsum) completed!\")\n",
    "\n",
    "    # Update results\n",
    "    results[\"wise_lipsum\"] = {\"id\": wise_lipsum_id_accuracy, \"ood\": wise_lipsum_ood_accuracies}\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"WISE-FT (LIPSUM) RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"WiSE-FT (Lipsum) ID Accuracy: {wise_lipsum_id_accuracy:.4f}\")\n",
    "print(f\"Best alpha: {best_alpha_lipsum}\")\n",
    "print(\"WiSE-FT (Lipsum) OOD Accuracies:\")\n",
    "for domain, acc in wise_lipsum_ood_accuracies.items():\n",
    "    print(f\"  {domain.capitalize()}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1823972",
   "metadata": {},
   "source": [
    "## Pareto front (ID-OOD plots)\n",
    "\n",
    "Now, when all the methods are trained, put them on a single Pareto front."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3498396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Print results summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for method_name, method_results in results.items():\n",
    "    print(f\"\\n{method_name.upper()}:\")\n",
    "    print(f\"  ID Accuracy: {method_results['id']:.4f}\")\n",
    "    print(f\"  OOD Accuracies:\")\n",
    "    for domain, acc in method_results['ood'].items():\n",
    "        print(f\"    {domain.capitalize()}: {acc:.4f}\")\n",
    "    avg_ood = np.mean(list(method_results['ood'].values()))\n",
    "    print(f\"  Average OOD: {avg_ood:.4f}\")\n",
    "\n",
    "# Create Pareto front plots for each domain\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "colors = {'zero_shot': 'blue', 'fine_tuned': 'orange', 'lipsum_ft': 'green', \n",
    "          'wise_regular': 'red', 'wise_lipsum': 'purple'}\n",
    "markers = {'zero_shot': 'o', 'fine_tuned': 's', 'lipsum_ft': '^', \n",
    "           'wise_regular': 'D', 'wise_lipsum': 'v'}\n",
    "\n",
    "for i, domain in enumerate(ood_domains):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Plot each method\n",
    "    for method_name, method_results in results.items():\n",
    "        id_acc = method_results['id']\n",
    "        ood_acc = method_results['ood'][domain]\n",
    "        \n",
    "        ax.scatter(id_acc, ood_acc, color=colors[method_name], marker=markers[method_name], \n",
    "                  s=100, alpha=0.7, edgecolors='black', linewidth=1,\n",
    "                  label=method_name.replace('_', ' ').title())\n",
    "    \n",
    "    ax.set_xlabel('ID Accuracy (Real)')\n",
    "    ax.set_ylabel(f'OOD Accuracy ({domain.capitalize()})')\n",
    "    ax.set_title(f'Pareto Front: Real → {domain.capitalize()}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "# Hide the last subplot\n",
    "axes[5].set_visible(False)\n",
    "\n",
    "plt.suptitle('ID vs OOD Accuracy Pareto Fronts', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary plot comparing all methods\n",
    "methods = list(results.keys())\n",
    "id_accs = [results[method]['id'] for method in methods]\n",
    "avg_ood_accs = [np.mean(list(results[method]['ood'].values())) for method in methods]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, id_accs, width, label='ID Accuracy', alpha=0.8)\n",
    "plt.bar(x + width/2, avg_ood_accs, width, label='Average OOD Accuracy', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Methods')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Method Comparison: ID vs Average OOD Accuracy')\n",
    "plt.xticks(x, [method.replace('_', ' ').title() for method in methods], rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPareto front analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e7d904",
   "metadata": {},
   "source": [
    "## Bonus: Fine-tuning data-efficiency [2 pts]\n",
    "\n",
    "Create a data-efficiency plot similar to Figure 6 from the [CLIP paper](https://arxiv.org/pdf/2103.00020) for the considered fine-tuning methods (regular vs. Lipsum-FT). On the horizontal axis you will have the number of fine-tuning samples per class (in logarithmic scale) and on the vertical axis &mdash; ID accuracy. What conclusions can be drawn about the data-efficiency of these methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0b98e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: Data efficiency experiment\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Check if we have training data\n",
    "if train_subset is None:\n",
    "    print(\"No training data available. Skipping data efficiency experiment.\")\n",
    "    print(\"Using placeholder results...\")\n",
    "    \n",
    "    # Create placeholder results\n",
    "    samples_per_class_list = [10, 25, 50, 100, 200]\n",
    "    regular_efficiency_results = [(s, 0.3 + s*0.001) for s in samples_per_class_list]\n",
    "    lipsum_efficiency_results = [(s, 0.35 + s*0.001) for s in samples_per_class_list]\n",
    "else:\n",
    "    # Define different data ratios to test\n",
    "    data_ratios = [0.1, 0.2, 0.33, 0.5, 1.0]  # 10%, 20%, 33%, 50%, 100%\n",
    "\n",
    "    regular_efficiency_results = []\n",
    "    lipsum_efficiency_results = []\n",
    "\n",
    "    print(\"Running data efficiency experiment...\")\n",
    "    print(\"This may take some time as we train multiple models...\")\n",
    "\n",
    "    for ratio in data_ratios:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training with {ratio*100:.0f}% of data\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Create subset of training data\n",
    "        subset_size = int(len(train_subset) * ratio)\n",
    "        subset_indices = np.random.choice(len(train_subset), subset_size, replace=False)\n",
    "        subset_dataset = Subset(train_subset, subset_indices)\n",
    "        subset_dataloader = DataLoader(subset_dataset, batch_size=16, shuffle=True, collate_fn=custom_collate_fn)\n",
    "        \n",
    "        samples_per_class = subset_size // len(class_names)\n",
    "        \n",
    "        # Regular fine-tuning with subset\n",
    "        print(f\"Regular FT with ~{samples_per_class} samples per class...\")\n",
    "        ft_model_subset = FineTunedCLIP(model, len(class_names)).to(device)\n",
    "        optimizer_subset = optim.AdamW(ft_model_subset.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        ft_model_subset.train()\n",
    "        for epoch in range(2):  # Reduced epochs for efficiency\n",
    "            for batch in tqdm(subset_dataloader, desc=f\"Regular FT Epoch {epoch+1}\"):\n",
    "                try:\n",
    "                    images = batch['image'].to(device)\n",
    "                    labels = batch['label'].to(device)\n",
    "                    \n",
    "                    optimizer_subset.zero_grad()\n",
    "                    outputs = ft_model_subset(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer_subset.step()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in training: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        # Evaluate regular FT\n",
    "        regular_acc = evaluate_finetuned(ft_model_subset, real_test_subset, \"Regular FT\")\n",
    "        regular_efficiency_results.append((samples_per_class, regular_acc))\n",
    "        print(f\"Regular FT Accuracy: {regular_acc:.4f}\")\n",
    "        \n",
    "        # Lipsum-FT with subset\n",
    "        print(f\"Lipsum-FT with ~{samples_per_class} samples per class...\")\n",
    "        lipsum_model_subset = LipsumFTCLIP(model, len(class_names)).to(device)\n",
    "        optimizer_lipsum_subset = optim.AdamW(lipsum_model_subset.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "        criterion_lipsum = nn.CrossEntropyLoss(reduction='none')\n",
    "        \n",
    "        lipsum_model_subset.train()\n",
    "        for epoch in range(2):  # Reduced epochs for efficiency\n",
    "            for batch in tqdm(subset_dataloader, desc=f\"Lipsum FT Epoch {epoch+1}\"):\n",
    "                try:\n",
    "                    images = batch['image'].to(device)\n",
    "                    labels = batch['label'].to(device)\n",
    "                    \n",
    "                    optimizer_lipsum_subset.zero_grad()\n",
    "                    \n",
    "                    # Extract features and generate Lipsum data\n",
    "                    with torch.no_grad():\n",
    "                        features = lipsum_model_subset.visual(images)\n",
    "                    \n",
    "                    mixed_features, lipsum_mask = generate_lipsum_data(features, features.size(0), 0.5)\n",
    "                    outputs = lipsum_model_subset.classifier(mixed_features)\n",
    "                    \n",
    "                    losses = criterion_lipsum(outputs, labels)\n",
    "                    real_mask = ~lipsum_mask\n",
    "                    real_loss = losses[real_mask].mean() if real_mask.any() else torch.tensor(0.0)\n",
    "                    lipsum_loss = losses[lipsum_mask].mean() if lipsum_mask.any() else torch.tensor(0.0)\n",
    "                    total_loss = real_loss + lipsum_loss\n",
    "                    \n",
    "                    total_loss.backward()\n",
    "                    optimizer_lipsum_subset.step()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in Lipsum training: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        # Evaluate Lipsum-FT\n",
    "        lipsum_acc = evaluate_finetuned(lipsum_model_subset, real_test_subset, \"Lipsum FT\")\n",
    "        lipsum_efficiency_results.append((samples_per_class, lipsum_acc))\n",
    "        print(f\"Lipsum-FT Accuracy: {lipsum_acc:.4f}\")\n",
    "\n",
    "# Plot data efficiency results\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "regular_samples, regular_accs = zip(*regular_efficiency_results)\n",
    "lipsum_samples, lipsum_accs = zip(*lipsum_efficiency_results)\n",
    "\n",
    "plt.plot(regular_samples, regular_accs, 'o-', label='Regular Fine-tuning', linewidth=2, markersize=8)\n",
    "plt.plot(lipsum_samples, lipsum_accs, '^-', label='Lipsum-FT', linewidth=2, markersize=8)\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Samples per Class (log scale)')\n",
    "plt.ylabel('ID Accuracy (Real)')\n",
    "plt.title('Data Efficiency: Regular Fine-tuning vs Lipsum-FT')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Print results\n",
    "print(\"\\nData Efficiency Results:\")\n",
    "print(\"Samples/Class | Regular FT | Lipsum-FT\")\n",
    "print(\"-\" * 40)\n",
    "for (reg_samples, reg_acc), (lip_samples, lip_acc) in zip(regular_efficiency_results, lipsum_efficiency_results):\n",
    "    print(f\"{reg_samples:11d} | {reg_acc:10.4f} | {lip_acc:9.4f}\")\n",
    "\n",
    "print(\"\\nConclusions:\")\n",
    "print(\"1. Compare the slopes of both curves to see which method learns faster with less data\")\n",
    "print(\"2. Observe which method achieves better performance with limited training data\")\n",
    "print(\"3. Note the performance gap at different data scales\")\n",
    "print(\"\\nData efficiency analysis completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
